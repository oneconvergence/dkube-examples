{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,json\n",
    "!{sys.executable} -m pip install kfp==1.4.0 kfp-server-api==1.2.0 --user >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MM DETAILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"details.txt\",\"r\") as f:\n",
    "    mm_details = json.load(f)\n",
    "\n",
    "mm_name = mm_details['MONITOR_NAME']\n",
    "data_source = mm_details['DATA_SOURCE']\n",
    "input_train_type = mm_details['INPUT_TRAIN_TYPE']\n",
    "OUTSIDE_DKUBE = mm_details['OUTSIDE_DKUBE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kfp import components\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "import os\n",
    "import json\n",
    "import kfp\n",
    "import string\n",
    "import random\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kfp import components\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "from dkube.sdk.api import DkubeApi\n",
    "from dkube.sdk.rsrcs import DkubeCode\n",
    "from dkube.sdk.rsrcs import DkubeDataset\n",
    "from dkube.sdk.rsrcs import DkubeModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dkube_preprocessing_op = components.load_component_from_file(\"/mnt/dkube/pipeline/components/preprocess/component.yaml\")\n",
    "dkube_training_op = components.load_component_from_file(\"/mnt/dkube/pipeline/components/training/component.yaml\")\n",
    "dkube_serving_op  = components.load_component_from_file(\"/mnt/dkube/pipeline/components/serving/component.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OUTSIDE_DKUBE:\n",
    "    token = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\")\n",
    "    username =  os.getenv(\"USER\")\n",
    "    url = os.getenv(\"DKUBE_URL\")\n",
    "\n",
    "else:\n",
    "    print(\"Taking information from details.txt file\")\n",
    "    token = mm_details['TOKEN']\n",
    "    username = mm_details['DKUBEUSERNAME']\n",
    "    url = mm_details['URL']\n",
    "\n",
    "api = DkubeApi(URL=url,token=token)\n",
    "client = kfp.Client(host=os.getenv(\"KF_PIPELINES_ENDPOINT\"), existing_token=token, namespace=os.getenv(\"USER\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_program = 'insurance'\n",
    "if (data_source == 'local' or data_source == 'aws_s3') and input_train_type == 'retraining':\n",
    "    input_training_dataset = mm_name+'-groundtruth'\n",
    "elif data_source == 'sql':\n",
    "    input_training_dataset = 'insurance-sql-data'\n",
    "else:\n",
    "    input_training_dataset = 'insurance-data'\n",
    "\n",
    "\n",
    "## Preprocessing stage inputs\n",
    "preprocessing_script =f\"pip3 install pymysql --user;python insurance/preprocessing.py --data_source {data_source} --train_type {input_train_type} --monitor_name {mm_name} --user {username}\"\n",
    "input_dataset_mount = ['/data']\n",
    "output_dataset = 'insurance-training-data'\n",
    "output_mount_path = ['/train-data']\n",
    "\n",
    "## Training stage inputs\n",
    "training_script = \"python insurance/training.py\"\n",
    "model_name = 'insurance-model'\n",
    "output_model_mount = \"/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='training-pipeline',\n",
    "    description='insurance-training-pl'\n",
    ")\n",
    "def insurance_pipeline(token):\n",
    "    \n",
    "    preprocessing = dkube_preprocessing_op(\n",
    "                                    auth_token=str(token),\n",
    "                                    container=json.dumps({\"image\": \"ocdr/d3-datascience-sklearn:v0.23.2-1\"}),\n",
    "                                    program=str(training_program),\n",
    "                                    datasets = json.dumps([str(input_training_dataset)]),\n",
    "                                    input_dataset_mounts = json.dumps(input_dataset_mount),\n",
    "                                    run_script=str(preprocessing_script),\n",
    "                                    outputs=json.dumps([str(output_dataset)]),\n",
    "                                    output_mounts=json.dumps(output_mount_path)).set_display_name(\"data-generation\")\n",
    "    \n",
    "    train       = dkube_training_op(container=json.dumps({\"image\": \"docker.io/ocdr/d3-datascience-sklearn:v0.23.2\"}),\n",
    "                                    framework=\"sklearn\", version=\"0.23.2\",\n",
    "                                    program=str(training_program), \n",
    "                                    run_script=str(training_script),\n",
    "                                    datasets=json.dumps([str(output_dataset)]), outputs=json.dumps([str(model_name)]),\n",
    "                                    input_dataset_mounts=json.dumps(output_mount_path),\n",
    "                                    output_mounts=json.dumps([str(output_model_mount)]),\n",
    "                                    auth_token=token).after(preprocessing)\n",
    "    \n",
    "    serving     = dkube_serving_op(model=train.outputs['artifact'], device='cpu', \n",
    "                                    serving_image=json.dumps({\"image\": \"ocdr/sklearnserver:0.23.2\"}),\n",
    "                                    transformer_image =json.dumps({\"image\": \"docker.io/ocdr/d3-datascience-sklearn:v0.23.2\"}),\n",
    "                                    transformer_project=str(training_program),\n",
    "                                    transformer_code='insurance/transformer.py', auth_token=token).after(train)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_run_from_pipeline_func(insurance_pipeline, arguments={'token':token})"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
