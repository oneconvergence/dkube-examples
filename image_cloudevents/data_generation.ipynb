{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import cv2\n",
    "import requests, json\n",
    "from numpy import random\n",
    "import datetime\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import boto3\n",
    "from dkube.sdk.api import DkubeApi\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONITOR_NAME = image_exp_config['MONITOR_NAME']\n",
    "DKUBEUSERNAME = image_exp_config['DKUBEUSERNAME']\n",
    "TOKEN = image_exp_config['TOKEN']\n",
    "DKUBE_URL = image_exp_config['DKUBE_URL']\n",
    "MINIO_KEY = image_exp_config['MINIO_KEY']\n",
    "MINIO_SECRET_KEY = image_exp_config['MINIO_SECRET_KEY']\n",
    "MINIO_ENDPOINT = image_exp_config['MINIO_ENDPOINT']\n",
    "RUN_FREQUENCY = image_exp_config['RUN_FREQUENCY']\n",
    "INFERENCE_URL = image_exp_config['INFERENCE_URL']\n",
    "DEPLOYMENT_ID = image_exp_config['DEPLOYMENT_ID']\n",
    "MINIO_BUCKET = image_exp_config['MINIO_BUCKET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_monitoring_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_types = ('.jpg', 'jpeg', '.png', '.svg')\n",
    "\n",
    "class ImageData():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def read_data_from_dir(self, imagedir, grayscale=True, read_labels=False):\n",
    "        image_files = list()\n",
    "        for file_type in image_types:\n",
    "            image_files.extend(glob.glob(os.path.join(imagedir, \"**/*\" + file_type), recursive=True))\n",
    "        if len(image_files) == 0:\n",
    "            return None\n",
    "        images = []\n",
    "        for each_image_file in image_files:\n",
    "            if grayscale:\n",
    "                img = cv2.imread(each_image_file, cv2.IMREAD_GRAYSCALE)\n",
    "            else:\n",
    "                img = cv2.imread(each_image_file)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "        train_x = np.asarray(images)\n",
    "        if read_labels:\n",
    "            csv_files = glob.glob(os.path.join(imagedir, \"**/*\" + \".csv\"), recursive=True)\n",
    "            label_data = pd.read_csv(csv_files[-1])\n",
    "            train_y = label_data.iloc[:,-1:].values\n",
    "            return train_x, train_y\n",
    "        else:\n",
    "            return train_x\n",
    "\n",
    "    def read_classification_data(self, datadir):\n",
    "        train_x = list()\n",
    "        train_y = list()\n",
    "        for dp, dn, filenames in os.walk(datadir):\n",
    "            if len(filenames) > 0:\n",
    "                current_class_data = self.read_data_from_dir(dp)\n",
    "                train_x.extend(current_class_data)\n",
    "                train_y.extend([os.path.basename(dp)] * current_class_data.shape[0])\n",
    "        if len(train_x) == 0:\n",
    "            return None\n",
    "        train_x = np.asarray(train_x)\n",
    "        train_y = np.asarray(train_y)\n",
    "        train_y_classes, train_y = np.unique(train_y, return_inverse=True)\n",
    "        return train_x, (train_y_classes, train_y)\n",
    "\n",
    "    def resize_images(self, images, new_shape):\n",
    "        resized_images = []\n",
    "        for each_image in images:\n",
    "            resized_images.append(cv2.resize(each_image, new_shape, interpolation= cv2.INTER_LINEAR))\n",
    "        resized_images = np.asarray(resized_images)\n",
    "        return resized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataGenerator:\n",
    "    BUCKET = None\n",
    "    S3_CLIENT = None\n",
    "    DB_ENGINE = None\n",
    "    API_CLIENT = None\n",
    "    TOKEN = None\n",
    "    USERNAME = None\n",
    "    INFERENCE_URL = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_time: datetime.datetime = None,\n",
    "        frequency=\"1H\",\n",
    "        model_frequency=10,\n",
    "        duration: str = \"10:24:12\",\n",
    "        margin=180,\n",
    "    ):\n",
    "\n",
    "        self.frequency  = frequency\n",
    "        self.margin=margin\n",
    "        self.model_frequency = model_frequency\n",
    "            \n",
    "        self.duration = duration\n",
    "        klass = type(self)\n",
    "        if not klass.BUCKET:\n",
    "            klass.BUCKET = MINIO_BUCKET\n",
    "        if not klass.S3_CLIENT:\n",
    "            klass.S3_CLIENT = boto3.client(\"s3\", aws_access_key_id=MINIO_KEY,\n",
    "                                                 aws_secret_access_key=MINIO_SECRET_KEY,\n",
    "                                                 endpoint_url = MINIO_ENDPOINT)\n",
    "        if not klass.TOKEN:\n",
    "            klass.TOKEN = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\",TOKEN)\n",
    "        if not klass.USERNAME:\n",
    "            klass.USERNAME= DKUBEUSERNAME\n",
    "        if not klass.API_CLIENT:\n",
    "            klass.API_CLIENT = DkubeApi(URL=os.getenv('DKUBE_URL',DKUBE_URL),token=klass.TOKEN)\n",
    "\n",
    "        duration = self.duration.split(\"-\")\n",
    "        if len(duration) < 2:\n",
    "            duration.append(\"0\")\n",
    "            duration.append(\"0\")\n",
    "        elif len(duration) < 3:\n",
    "            duration.append(\"0\")\n",
    "        \n",
    "    def save_dataset(self, data, data_name:str, s3_prefix):\n",
    "        klass = type(self)\n",
    "        return klass.save_dataset_to_s3(data, data_name, s3_prefix)\n",
    "    \n",
    "    @classmethod\n",
    "    def save_dataset_to_s3(cls, data, name, s3_prefix):\n",
    "        file_name = name + \".csv\"\n",
    "        file_path = os.path.join(s3_prefix, file_name)\n",
    "        with io.StringIO() as csv_buffer:\n",
    "            data.to_csv(csv_buffer, index=False)\n",
    "            response = cls.S3_CLIENT.put_object(\n",
    "                Bucket=cls.BUCKET, Key=file_path, Body=csv_buffer.getvalue()\n",
    "            )\n",
    "            status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "            if status == 200:\n",
    "                print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "                return file_path\n",
    "            else:\n",
    "                print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n",
    "                    \n",
    "    @property\n",
    "    def frequency_ts(self):\n",
    "        value = int(self.frequency[:-1])\n",
    "        unit = self.frequency[-1].lower()\n",
    "        seconds_per_unit = {\"s\": 1, \"m\": 60, \"h\": 3600, \"d\": 86400, \"w\": 604800}\n",
    "        seconds_count = int(value) * seconds_per_unit[unit]\n",
    "        now = datetime.datetime.utcnow()\n",
    "        if unit.lower() == \"h\":\n",
    "            delta = datetime.timedelta(hours=value)\n",
    "            new_time = (now+delta).replace(minute = 0, second =0, microsecond=0) - datetime.timedelta(seconds=self.margin)\n",
    "            second_remaining = (new_time-now).seconds\n",
    "            result =  seconds_count if second_remaining > seconds_count or second_remaining == 0 else second_remaining\n",
    "            print(f\"Next Push after {datetime.timedelta(seconds=result)}\")\n",
    "            return result        \n",
    "        elif unit == \"m\":\n",
    "            diff = abs(now.minute%-value)\n",
    "            if diff == 0:\n",
    "                delta = datetime.timedelta(minutes=value)\n",
    "                new_time = (now+delta).replace(second =0, microsecond=0) - datetime.timedelta(seconds=self.margin)\n",
    "                result = (new_time-now).seconds\n",
    "                print(f\"Next Push after {datetime.timedelta(seconds=result)}\")\n",
    "                return result\n",
    "            else:\n",
    "                delta = datetime.timedelta(minutes = diff)\n",
    "                new_time = (now+delta).replace(second =0, microsecond=0) - datetime.timedelta(seconds=self.margin)\n",
    "                if new_time < now:\n",
    "                    new_time = new_time + datetime.timedelta(minutes=value)\n",
    "                second_remaining = (new_time-now).seconds\n",
    "                result =  seconds_count if second_remaining > seconds_count or second_remaining == 0 else second_remaining\n",
    "                print(f\"Next Push after {datetime.timedelta(seconds=result)}\")\n",
    "                return result\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def awsS3Secret(self):\n",
    "        if DATA_SOURCE == 'aws_s3':\n",
    "            AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY_ID\",ACCESS_KEY) \n",
    "            AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\",SECRET_KEY)\n",
    "            print(AWS_ACCESS_KEY)\n",
    "        if AWS_ACCESS_KEY and AWS_SECRET_KEY:\n",
    "            return {\"access_key\":AWS_ACCESS_KEY, \"secret_key\": AWS_SECRET_KEY}\n",
    "        else:\n",
    "            home_dir = os.getenv(\"HOME\")\n",
    "            if home_dir:\n",
    "                creds_path = os.path.join(home_dir, \".aws\",\"credentials\")\n",
    "                config = ConfigParser()\n",
    "                if os.path.isfile(creds_path):\n",
    "                    config.read(creds_path)\n",
    "                    if \"default\" in config:\n",
    "                        AWS_ACCESS_KEY = config[\"default\"][\"aws_access_key_id\"]\n",
    "                        AWS_SECRET_KEY = config[\"default\"][\"aws_secret_access_key\"]\n",
    "                        if AWS_ACCESS_KEY and AWS_SECRET_KEY:\n",
    "                            return {\"access_key\":AWS_ACCESS_KEY, \"secret_key\": AWS_SECRET_KEY}\n",
    "                \n",
    "        \n",
    "    @property\n",
    "    def end(self):\n",
    "        duration = self.duration.split(\":\")\n",
    "        if len(duration) < 2:\n",
    "            duration.append(\"0\")\n",
    "            duration.append(\"0\")\n",
    "        elif len(duration) < 3:\n",
    "            duration.append(\"0\")\n",
    "        return self.start_time + datetime.timedelta(\n",
    "            hours=int(duration[0]), minutes=int(duration[1]), seconds=int(duration[2])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageDataGenerator.URL = DKUBE_URL\n",
    "ImageDataGenerator.TOKEN = TOKEN\n",
    "ImageDataGenerator.API_CLIENT = DkubeApi(URL=DKUBE_URL, token=TOKEN)\n",
    "if INFERENCE_URL is not None:\n",
    "    ImageDataGenerator.INFERENCE_URL = INFERENCE_URL\n",
    "else:\n",
    "     raise \"INFERENCE_URL is Empty, Provide value for variable INFERENCE_URL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ImageDataGenerator(MONITOR_NAME,\n",
    "                                   frequency=f\"{RUN_FREQUENCY}m\",\n",
    "                                   model_frequency = RUN_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imd = ImageData()\n",
    "train_x, train_y = imd.read_classification_data(\"data/\")\n",
    "train_y_classes, train_y = train_y\n",
    "resized_train_x = imd.resize_images(train_x, (200,200))\n",
    "resized_train_x = resized_train_x.reshape(resized_train_x.shape[0], 200, 200, 1)\n",
    "resized_train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(resized_train_x.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "resized_train_x = resized_train_x[indices]\n",
    "train_y = train_y[indices]\n",
    "resized_train_x.shape, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_url = INFERENCE_URL\n",
    "token = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal = lambda n: \"%d%s\" % (n,\"tsnrhtdd\"[(n//10%10!=1)*(n%10<4)*n%10::4])\n",
    "push_count = 1\n",
    "for i in range(no_of_monitoring_runs):\n",
    "    ## Sending 10 samples at a time. \n",
    "    outputs = []\n",
    "    labels = []\n",
    "    second_remaining = generator.frequency_ts\n",
    "    time.sleep(second_remaining)\n",
    "    no_of_samples = random.randint(10,15)\n",
    "    print(\"Generating data\")\n",
    "    for i in range(no_of_samples):\n",
    "        ch = random.choice(range(resized_train_x.shape[0]))\n",
    "        if i%2:\n",
    "            x = resized_train_x[ch:ch+1]\n",
    "        else:\n",
    "            x = resized_train_x[ch:ch+1].T # rotating image for drift\n",
    "        payload = {\n",
    "            \"inputs\": {'input_1': x.tolist()}\n",
    "        }\n",
    "        r = requests.post(predict_url, json=payload, headers = {'authorization': \"Bearer \" + token}, verify = False)\n",
    "        prediction = json.loads(r.content.decode('utf-8'))\n",
    "        each_output = np.array(prediction[\"outputs\"])\n",
    "        each_output = train_y_classes[each_output.argmax(axis=1)].tolist()\n",
    "        each_label = train_y_classes[train_y[ch:ch+1]].tolist()\n",
    "        outputs.extend(each_output)\n",
    "        labels.extend(each_label)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    start = datetime.datetime.utcnow()\n",
    "    end = start + datetime.timedelta(seconds=10)\n",
    "    timestamps = pd.date_range(start, end, len(outputs))\n",
    "    labelled_df = pd.DataFrame({\n",
    "        \"timestamp\": timestamps,\n",
    "        \"output\": outputs,\n",
    "        \"label\": labels\n",
    "    })   \n",
    "    filename = f\"lablled_data_{i+1}\"\n",
    "    g_path = generator.save_dataset(labelled_df, filename, DEPLOYMENT_ID + \"/livedata\")\n",
    "    if g_path:\n",
    "        print(g_path)\n",
    "    print(f\"Pushed data for {ordinal(push_count)} time, Remaining pushes: {no_of_monitoring_runs-push_count}, Monitor name: {MONITOR_NAME}\")\n",
    "    push_count += 1\n",
    "print(\"***************** DATA GENERATION COMPLETED ******************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
