# Chest X-Ray Image Example

 This example trains a model to identify pneumonia from chest x-rays.  The model is then deployed and used as the basis for monitoring with synthetic live data to demonstrate the DKube monitoring capability.

- This example only supports predict dataset sources as **CloudEvents**. 
- This example  supports model deployment with a full DKube cluster (`serving cluster`) and model monitoring on either the same cluster or a seperate minimal DKube cluster (:`monitoring cluster`).
  - **Serving cluster:** Where the production deployment will be running
  - **Monitoring cluster:** Where the model monitor will be running
  > **Note**: The serving and monitoring clusters can be same, but in that case the setup has to be a single **full** DKube setup

## Example Flow

- Create the necessary DKube resources
  - This includes the Code, Dataset and Model repos
- Train a model for the example using Tensorflow
- Deploy the model for serving
- Create a monitor
- For seperate serving and monitoring clusters
  - Add a serving cluster link on the monitoring cluster
  - Import the deployment onto the monitoring cluster
- Generate data for analysis by the monitor
  - Predict data: Inference inputs/outputs
  - Label data:  Dataset with Groundtruth values for the inferences generated above
  > **Note** In a production environment, the label data would be manually generated by experts in the domain.  In this example, we are creating the labeled data automatically.
- Cleanup the resources after the example is complete

## 1. Create DKube Code Repo

 The only manually created resource requirement for this example is the Code repo.  The rest of the resources are created by the notebook script.

 - Select `Code` menu on the left, then `+ Code`, and fill in the following fields:
   - **Name:** `monitoring-examples`  **(Or choose your own name as `<your-code-repo>`)**
   - **Source:** `Git`
   - **URL:** `https://github.com/oneconvergence/dkube-examples.git`
   - **Branch:** `monitoring`
 - Leave the rest of the fields at their current value
 - `Add Code`

## 2. Create and Launch JupyterLab

 In order to run the script to set up the resources, train and deploy the model, and create the monitor, a JupyterLab IDE needs to be created.  The scripts will be run from within JupyterLab.

 - Select `IDE` menu on the left, then `+ JupyterLab`, and fill in the following fields:
   - **Name:** *`<your-IDE-name>`*  **(Choose a name)**
   - **Code:** *`<your-code-repo>`*  **(Chosen during Code repo creation)**
   - **Framework:** `tensorflow`
   - **Framework Version:** `2.0.0`
   - **Image:** `ocdr/dkube-datascience-tf-cpu-multiuser:v2.0.0-17`   **(This should be the default, but ensure that it is selected)**
 - Leave the rest of the fields at their current value
 - `Submit`

 ## 3. Create the Resources

 - Once the IDE is running, launch JupyterLab from the icon on the far right
 - Navigate to <code>workspace/**\<your-code-repo\>**/image_cloudevents</code>
 - Open `resources.ipynb`
 > **Warning** Ensure that `Cleanup = False` in the last cell, since it may have been changed in a previous execution

 - If you called your code repo something other than `monitoring-examples`, edit the following variable:
   - `DKUBE_TRAINING_CODE_NAME` = *`<your-code-repo>`*
 
### Serving and Monitoring on Same Cluster

 - If the serving and monitoring cluster are the same, no other fields needs to be changed, skip to `Run the Script`

### Serving and Monitoring on Different Clusters

 - If the monitoring cluster is separate from the serving cluster, you need to provide more information for cluster communication
   - `SERVING_CLUSTER_EXECUTION` = `False`
   - `SERVING_DKUBE_URL` = DKube access URL for the serving cluster, with the form
     - `https://<Serving Cluster Access IP Address>:32222/`
     > **Note** Ensure that there is a final `/` in the URL field <br><br>
   - `MONITOR_DKUBE_URL `= DKube access URL from the monitoring cluster, with the form
   - `MONITORING_DKUBE_USERNAME` = Username on the monitoring cluster
   - `MONITORING_DKUBE_TOKEN` = Authentication token from the monitoring cluster, from the `Developer Settings` menu
     - `https://<Monitor Cluster Access IP Address>:32222/`
     > **Note** Ensure that there is a final `/` in the URL field
 - If the Monitoring cluster already has a link to the Serving cluster from the DKube Clusters Operator screen
   - Get the name of the DKube cluster link and provide that name to the variable `SERVING_DKUBE_CLUSTER_NAME` <br><br>
 - If the Monitoring cluster link has not been created by the Operator on the Monitoring cluster:
   - Leave the variable `SERVING_DKUBE_CLUSTER_NAME = ""`
   - In that case, the link will be created on the Monitoring cluster
   - The username identified in the `MONITORING_DKUBE_USERNAME` variable must have Operator privileges for this to work. If not, the script fill fail.
   - Leave the other fields at their current value

### Run the Script

 - `Run` > `Run All Cells` from the top menu  

<!---
This is from the original readme.  I am leaving it here for reference for enhancements later

4. Open Jupyterlab and from **workspace/monitoring-examples/image_cloudevents** open [resources.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/image_cloudevents/resources.ipynb) and fill the following details in the first cell.
    - In case of running the example notebook other than the serving setup, In the 1st cell, set RUNNING_IN_SAME to False and Fill the below details,
    - **SERVING_DKUBE_URL** = {DKube url of serving cluster}
    - **SERVING_DKUBE_USERNAME** = {DKube username of serving cluster}
    - **SERVING_DKUBE_TOKEN** = {DKube authentication token of serving cluster}
    - if there is a sperate monitoring cluster then also fill the below details, otherwise leave these value empty.
      - **MONITORING_DKUBE_USERNAME** = {Dkube username of monitoring cluster}
      - **MONITORING_DKUBE_TOKEN** = {DKube authentication token of monitoring cluster}
      - **MONITORING_DKUBE_URL** = {DKube URL of monitoring cluster}
    - **MONITOR_NAME** = {model monitor name}
    - **MINIO_KEY** = {MINIO access key of Dkube setup where the prediction deployment is running}
    - **MINIO_SECRET_KEY** = {MINIO access secret key of Dkube setup where the prediction deployment is running}
      - MINIO_KEY and MINIO_SECRET_KEY values will be filled automatically by the example with SDK call, these values can also be obtained by running the following commands on the DKube setup where the prediction deployment is running. Provide the creds manually if the user is neither PE nor Operator on the remote cluster.
        - DKube API. Fill in DKUBE_IP and TOKEN in the following curl command
          - `curl -X 'GET' \
              'https://DKUBE_IP:32222/dkube/v2/controller/v2/deployments/logstore' \
              -H 'accept: application/json' \
              -H 'Authorization: Bearer <TOKEN>'`
        - If you have access to Kubernetes, you can get the secrets by running the following commands
          - `kubectl get secret -n dkube-infra cloudevents-minio-secret -o jsonpath="{.data.AWS_ACCESS_KEY_ID}" | base64 -d`
          - `kubectl get secret -n dkube-infra cloudevents-minio-secret -o jsonpath="{.data.AWS_SECRET_ACCESS_KEY}" | base64 -d`
    - The following will be derived from the environment automatically if the notebook is running inside same Dkube IDE. Otherwise in case if the notebook is running locally or in other Dkube Setup , then please fill in, 
5. Run all the cells. This will create all the DKube resources required for this example automatically. In case of seperate serving and monitoring cluster, the required resources will be created on the respective cluster.
-->

 - The following resources will be created:
   - `chest-xray` Dataset on both the serving and monitoring cluster
   - `image-mm-kf` Dataset on the serving cluster
   - `image-mm-kf-s3` Dataset on the monitoring cluster

## 4. Train and Deploy the Model on Serving Cluster

 In order for monitor example to operate, a model must be trained and deployed on the serving cluster.  A Kubeflow Pipeline executes this step.

 - Open `train.ipynb`
 - `Run All Cells`
 - This creates and executes a pipeline in order to:
   - Preprocess the dataset and generate the training data or retraining data
   - Train with the generated dataset as an input, and create an output model
   - Deploy the generated model on a predict endpoint
 - The pipeline will create a new version of the Model `image-mm-kf`
 > **Note** Wait for the pipeline to complete before continuing

## 5. Create a Model Monitor

 In order to monitor the deployed model, a monitor is created and launched.  This workflow executes this programatically through the DKube SDK. This can also be done through the UI by following a different example flow in this repo.

 - Open `modelmonitor.ipynb`
 
 > **Warning** Ensure that `Cleanup = False` in the last cell, since it may have been changed in a previous execution
 
 - `Run all Cells`
 - This script will:
   - Add the right links and import the deployment if the monitoring is on a different cluster from the serving cluster
   - Create a new model monitor
 - After the script has completed, the monitor `image-mm-kf` will be in the active state

## 6. Generate Data

 Predict and Groundtruth datasets will be generated by this script, and will be used by the monitor to analyse the model execution.

  - Open `data_generation.ipynb`
  - In the 1st cell, update the number of data generation cycles to complete
  - `Run All Cells`
  - This will start to push the data based on the definitions generated in the previous `resources.ipynb` file

  > **Note** Live data will be created on the MinIO server under the deployment ID.

## 7. Clean Up the Data when Complete

 After you are done with the example, clean up the data by running the `Cleanup` cells in the `modelmonitor` and `resources` scripts

 - Set the `Cleanup` variable to `True` in the last cell for each script, select that cell, and `Run Selected Cells` (not all cells)

 > **Warning** Ensure that you restore the `Cleanup` variable to `False` after completion, or the scripts will not work on the next execution
