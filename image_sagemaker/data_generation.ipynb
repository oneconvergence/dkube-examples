{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined by User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "## Number of times the script will push data\n",
    "no_of_monitoring_runs  = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import cv2\n",
    "import requests, json\n",
    "from numpy import random\n",
    "import datetime\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import boto3, sagemaker\n",
    "from dkube.sdk.api import DkubeApi\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_NAME = image_sgmkr_config['ENDPOINT_NAME']\n",
    "MONITOR_NAME = image_sgmkr_config['MONITOR_NAME'] \n",
    "ACCESS_KEY = image_sgmkr_config['ACCESS_KEY']\n",
    "SECRET_KEY = image_sgmkr_config['SECRET_KEY']\n",
    "BUCKET = image_sgmkr_config['BUCKET']\n",
    "REGION_NAME = image_sgmkr_config['REGION_NAME']\n",
    "RUN_FREQUENCY = image_sgmkr_config['RUN_FREQUENCY']\n",
    "TOKEN = image_sgmkr_config['TOKEN']\n",
    "DKUBE_URL = image_sgmkr_config['DKUBE_URL']\n",
    "DKUBEUSERNAME = image_sgmkr_config['DKUBEUSERNAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQUENCY = f\"{RUN_FREQUENCY}m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_types = ('.jpg', 'jpeg', '.png', '.svg')\n",
    "\n",
    "class ImageData():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def read_data_from_dir(self, imagedir, grayscale=True, read_labels=False):\n",
    "        image_files = list()\n",
    "        for file_type in image_types:\n",
    "            image_files.extend(glob.glob(os.path.join(imagedir, \"**/*\" + file_type), recursive=True))\n",
    "        if len(image_files) == 0:\n",
    "            return None\n",
    "        images = []\n",
    "        for each_image_file in image_files:\n",
    "            if grayscale:\n",
    "                img = cv2.imread(each_image_file, cv2.IMREAD_GRAYSCALE)\n",
    "            else:\n",
    "                img = cv2.imread(each_image_file)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "        train_x = np.asarray(images)\n",
    "        if read_labels:\n",
    "            csv_files = glob.glob(os.path.join(imagedir, \"**/*\" + \".csv\"), recursive=True)\n",
    "            label_data = pd.read_csv(csv_files[-1])\n",
    "            train_y = label_data.iloc[:,-1:].values\n",
    "            return train_x, train_y\n",
    "        else:\n",
    "            return train_x\n",
    "\n",
    "    def read_classification_data(self, datadir):\n",
    "        train_x = list()\n",
    "        train_y = list()\n",
    "        for dp, dn, filenames in os.walk(datadir):\n",
    "            if len(filenames) > 0:\n",
    "                current_class_data = self.read_data_from_dir(dp)\n",
    "                train_x.extend(current_class_data)\n",
    "                train_y.extend([os.path.basename(dp)] * current_class_data.shape[0])\n",
    "        if len(train_x) == 0:\n",
    "            return None\n",
    "        train_x = np.asarray(train_x)\n",
    "        train_y = np.asarray(train_y)\n",
    "        train_y_classes, train_y = np.unique(train_y, return_inverse=True)\n",
    "        return train_x, (train_y_classes, train_y)\n",
    "\n",
    "    def resize_images(self, images, new_shape):\n",
    "        resized_images = []\n",
    "        for each_image in images:\n",
    "            resized_images.append(cv2.resize(each_image, new_shape, interpolation= cv2.INTER_LINEAR))\n",
    "        resized_images = np.asarray(resized_images)\n",
    "        return resized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE_DATA_S3_PATH = \"https://dkube-examples-data.s3.us-west-2.amazonaws.com/monitoring-insurance/training-data/insurance.csv\"\n",
    "\n",
    "MODEL_FREQUENCY = RUN_FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataGenerator:\n",
    "    BUCKET = None\n",
    "    S3_CLIENT = None\n",
    "    DB_ENGINE = None\n",
    "    API_CLIENT = None\n",
    "    TOKEN = None\n",
    "    USERNAME = None\n",
    "    INFERENCE_URL = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_time: datetime.datetime = None,\n",
    "        frequency=\"1H\",\n",
    "        model_frequency=10,\n",
    "        duration: str = \"10:24:12\",\n",
    "        margin=180,\n",
    "    ):\n",
    "\n",
    "        self.frequency  = frequency\n",
    "        self.margin=margin\n",
    "        self.model_frequency = model_frequency\n",
    "            \n",
    "        self.duration = duration\n",
    "        klass = type(self)\n",
    "        if not klass.BUCKET:\n",
    "            klass.BUCKET = BUCKET\n",
    "        if not klass.S3_CLIENT:\n",
    "            klass.S3_CLIENT = boto3.client(\"s3\", aws_access_key_id=ACCESS_KEY,\n",
    "                                                 aws_secret_access_key=SECRET_KEY)\n",
    "        if not klass.TOKEN:\n",
    "            klass.TOKEN = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\",TOKEN)\n",
    "        if not klass.USERNAME:\n",
    "            klass.USERNAME= DKUBEUSERNAME\n",
    "        if not klass.API_CLIENT:\n",
    "            klass.API_CLIENT = DkubeApi(URL=os.getenv('DKUBE_URL',DKUBE_URL),token=klass.TOKEN)\n",
    "\n",
    "        duration = self.duration.split(\"-\")\n",
    "        if len(duration) < 2:\n",
    "            duration.append(\"0\")\n",
    "            duration.append(\"0\")\n",
    "        elif len(duration) < 3:\n",
    "            duration.append(\"0\")\n",
    "        \n",
    "    def save_dataset(self, data, data_name:str, s3_prefix):\n",
    "        klass = type(self)\n",
    "        return klass.save_dataset_to_s3(data, data_name, s3_prefix)\n",
    "    \n",
    "    @classmethod\n",
    "    def save_dataset_to_s3(cls, data, name, s3_prefix):\n",
    "        file_name = name + \".csv\"\n",
    "        file_path = os.path.join(s3_prefix, file_name)\n",
    "        with io.StringIO() as csv_buffer:\n",
    "            data.to_csv(csv_buffer, index=False)\n",
    "            response = cls.S3_CLIENT.put_object(\n",
    "                Bucket=cls.BUCKET, Key=file_path, Body=csv_buffer.getvalue()\n",
    "            )\n",
    "            status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "            if status == 200:\n",
    "                print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "                return file_path\n",
    "            else:\n",
    "                print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n",
    "                    \n",
    "    @property\n",
    "    def frequency_ts(self):\n",
    "        value = int(self.frequency[:-1])\n",
    "        unit = self.frequency[-1].lower()\n",
    "        seconds_per_unit = {\"s\": 1, \"m\": 60, \"h\": 3600, \"d\": 86400, \"w\": 604800}\n",
    "        seconds_count = int(value) * seconds_per_unit[unit]\n",
    "        now = datetime.datetime.utcnow()\n",
    "        if unit.lower() == \"h\":\n",
    "            delta = datetime.timedelta(hours=value)\n",
    "            new_time = (now+delta).replace(minute = 0, second =0, microsecond=0) - datetime.timedelta(seconds=self.margin)\n",
    "            second_remaining = (new_time-now).seconds\n",
    "            result =  seconds_count if second_remaining > seconds_count or second_remaining == 0 else second_remaining\n",
    "            print(f\"Next Push after {datetime.timedelta(seconds=result)}\")\n",
    "            return result        \n",
    "        elif unit == \"m\":\n",
    "            diff = abs(now.minute%-value)\n",
    "            if diff == 0:\n",
    "                delta = datetime.timedelta(minutes=value)\n",
    "                new_time = (now+delta).replace(second =0, microsecond=0) - datetime.timedelta(seconds=self.margin)\n",
    "                result = (new_time-now).seconds\n",
    "                print(f\"Next Push after {datetime.timedelta(seconds=result)}\")\n",
    "                return result\n",
    "            else:\n",
    "                delta = datetime.timedelta(minutes = diff)\n",
    "                new_time = (now+delta).replace(second =0, microsecond=0) - datetime.timedelta(seconds=self.margin)\n",
    "                if new_time < now:\n",
    "                    new_time = new_time + datetime.timedelta(minutes=value)\n",
    "                second_remaining = (new_time-now).seconds\n",
    "                result =  seconds_count if second_remaining > seconds_count or second_remaining == 0 else second_remaining\n",
    "                print(f\"Next Push after {datetime.timedelta(seconds=result)}\")\n",
    "                return result\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def awsS3Secret(self):\n",
    "        if DATA_SOURCE == 'aws_s3':\n",
    "            AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY_ID\",ACCESS_KEY) \n",
    "            AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\",SECRET_KEY)\n",
    "            print(AWS_ACCESS_KEY)\n",
    "        if AWS_ACCESS_KEY and AWS_SECRET_KEY:\n",
    "            return {\"access_key\":AWS_ACCESS_KEY, \"secret_key\": AWS_SECRET_KEY}\n",
    "        else:\n",
    "            home_dir = os.getenv(\"HOME\")\n",
    "            if home_dir:\n",
    "                creds_path = os.path.join(home_dir, \".aws\",\"credentials\")\n",
    "                config = ConfigParser()\n",
    "                if os.path.isfile(creds_path):\n",
    "                    config.read(creds_path)\n",
    "                    if \"default\" in config:\n",
    "                        AWS_ACCESS_KEY = config[\"default\"][\"aws_access_key_id\"]\n",
    "                        AWS_SECRET_KEY = config[\"default\"][\"aws_secret_access_key\"]\n",
    "                        if AWS_ACCESS_KEY and AWS_SECRET_KEY:\n",
    "                            return {\"access_key\":AWS_ACCESS_KEY, \"secret_key\": AWS_SECRET_KEY}\n",
    "                \n",
    "        \n",
    "    @property\n",
    "    def end(self):\n",
    "        duration = self.duration.split(\":\")\n",
    "        if len(duration) < 2:\n",
    "            duration.append(\"0\")\n",
    "            duration.append(\"0\")\n",
    "        elif len(duration) < 3:\n",
    "            duration.append(\"0\")\n",
    "        return self.start_time + datetime.timedelta(\n",
    "            hours=int(duration[0]), minutes=int(duration[1]), seconds=int(duration[2])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageDataGenerator.URL = DKUBE_URL\n",
    "ImageDataGenerator.TOKEN = TOKEN\n",
    "ImageDataGenerator.API_CLIENT = DkubeApi(URL=DKUBE_URL, token=TOKEN)\n",
    "if ENDPOINT_NAME is not None:\n",
    "    ImageDataGenerator.ENDPOINT_NAME = ENDPOINT_NAME\n",
    "else:\n",
    "     raise \"ENDPOINT_NAME is Empty, Provide value for variable ENDPOINT_NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ImageDataGenerator(MONITOR_NAME,\n",
    "                                   frequency=f\"{RUN_FREQUENCY}m\",\n",
    "                                   model_frequency = RUN_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imd = ImageData()\n",
    "train_x, train_y = imd.read_classification_data(\"data/\")\n",
    "train_y_classes, train_y = train_y\n",
    "resized_train_x = imd.resize_images(train_x, (200,200))\n",
    "resized_train_x = resized_train_x.reshape(resized_train_x.shape[0], 200, 200, 1)\n",
    "resized_train_x.shape\n",
    "\n",
    "indices = np.arange(resized_train_x.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "resized_train_x = resized_train_x[indices]\n",
    "train_y = train_y[indices]\n",
    "resized_train_x.shape, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wating for sagemaker deplyment to be ready\n",
    "client = boto3.client(service_name=\"sagemaker\",region_name=REGION_NAME)\n",
    "endpoint_name = ENDPOINT_NAME\n",
    "describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"Checked at {datetime.datetime.now()}, Sagemaker is creating the endpoint, wait ...\")\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(15)\n",
    "if describe_endpoint_response[\"EndpointStatus\"] == \"InService\":\n",
    "    print(\"Endpoint is ready\")\n",
    "else:\n",
    "    raise Exception(f\"Endpoint is in {describe_endpoint_response['EndpointStatus']} state\")\n",
    "\n",
    "\n",
    "ordinal = lambda n: \"%d%s\" % (n,\"tsnrhtdd\"[(n//10%10!=1)*(n%10<4)*n%10::4])\n",
    "push_count = 1\n",
    "for i in range(no_of_monitoring_runs):\n",
    "    ## Sending 10 samples at a time. \n",
    "    outputs = []\n",
    "    labels = []\n",
    "    second_remaining = generator.frequency_ts\n",
    "    time.sleep(second_remaining)\n",
    "    no_of_samples = random.randint(10,15)\n",
    "    print(\"Generating data\")\n",
    "    for i in range(no_of_samples):\n",
    "        ch = random.choice(range(resized_train_x.shape[0]))\n",
    "        if i%2:\n",
    "            x = resized_train_x[ch:ch+1]\n",
    "        else:\n",
    "            x = resized_train_x[ch:ch+1].T # rotating image for drift\n",
    "        payload = {\n",
    "            \"inputs\": {'input_1': x.tolist()}\n",
    "        }\n",
    "        r = requests.post(predict_url, json=payload, headers = {'authorization': \"Bearer \" + token}, verify = False)\n",
    "        prediction = json.loads(r.content.decode('utf-8'))\n",
    "        each_output = np.array(prediction[\"outputs\"])\n",
    "        each_output = train_y_classes[each_output.argmax(axis=1)].tolist()\n",
    "        each_label = train_y_classes[train_y[ch:ch+1]].tolist()\n",
    "        outputs.extend(each_output)\n",
    "        labels.extend(each_label)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    start = datetime.datetime.utcnow()\n",
    "    end = start + datetime.timedelta(seconds=10)\n",
    "    timestamps = pd.date_range(start, end, len(outputs))\n",
    "    labelled_df = pd.DataFrame({\n",
    "        \"timestamp\": timestamps,\n",
    "        \"output\": outputs,\n",
    "        \"label\": labels\n",
    "    })   \n",
    "    filename = f\"lablled_data_{i+1}\"\n",
    "    g_path = generator.save_dataset(labelled_df, filename, DEPLOYMENT_ID + \"/livedata\")\n",
    "    if g_path:\n",
    "        print(g_path)\n",
    "    print(f\"Pushed data for {ordinal(push_count)} time, Remaining pushes: {no_of_monitoring_runs-push_count}, Monitor name: {MONITOR_NAME}\")\n",
    "    push_count += 1\n",
    "print(\"***************** DATA GENERATION COMPLETED ******************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
