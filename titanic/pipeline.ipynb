{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import kfp\n",
    "from dkube.sdk import *\n",
    "from dkube.pipelines import dkube_training_op, dkube_preprocessing_op, dkube_serving_op, dkube_storage_op, dkube_submit_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\")\n",
    "client = kfp.Client(existing_token=token)\n",
    "api = DkubeApi(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_url = \"/mnt/dkube/pipeline/components/\"\n",
    "dkube_preprocessing_op = kfp.components.load_component_from_file(components_url + \"preprocess/component.yaml\")\n",
    "dkube_training_op = kfp.components.load_component_from_file(components_url + \"training/component.yaml\")\n",
    "dkube_job_op  = kfp.components.load_component_from_file(components_url + \"job/component.yaml\")\n",
    "dkube_submit_op = kfp.components.load_component_from_file(components_url + \"submit/component.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project owner resources\n",
    "project_id = os.environ.get(\"DKUBE_PROJECT_ID\")\n",
    "project_name = os.environ.get(\"DKUBE_PROJECT_NAME\")\n",
    "project_owner = os.environ.get(\"DKUBE_PROJECT_OWNER\")\n",
    "\n",
    "assert project_id != \"\", \"Please launch IDE under project or set above variables manually\"\n",
    "\n",
    "username = os.getenv(\"USERNAME\",\"ocdkube\")\n",
    "ptrain_dataset = f'{project_owner}:titanic-train'\n",
    "ptest_dataset = f'{project_owner}:titanic-test'\n",
    "\n",
    "# User resources\n",
    "code_name = f'{project_name}-code'\n",
    "train_fs_name = f\"{project_name}-train-fs-{username}\"\n",
    "test_fs_name = f\"{project_name}-test-fs-{username}\"\n",
    "model_name = f'{project_name}-model'\n",
    "\n",
    "# Program specific variables\n",
    "image = \"docker.io/ocdr/dkube-datascience-tf-cpu:v2.0.0-3\"\n",
    "dataset_mount_points = [\"/dataset/train\", \"/dataset/test\"]\n",
    "featureset_mount_points = [\"/featureset/train\", \"/featureset/test\"]\n",
    "preprocessing_script = f\"python titanic/preprocess.py --train_fs {train_fs_name} --test_fs {test_fs_name}\"\n",
    "training_script = \"python titanic/train.py\"\n",
    "predict_script = \"python titanic/predict.py\"\n",
    "train_out_mount_points = [\"/model\"]\n",
    "runid = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #set active project\n",
    "    api.set_active_project(project_id)\n",
    "    #Create code repo \n",
    "    code = DkubeCode(username,code_name)\n",
    "    code.update_git_details(url=\"https://github.com/oneconvergence/dkube-examples.git\", branch=\"tensorflow\")\n",
    "    api.create_code(code)\n",
    "    #Create train/test featureset\n",
    "    api.create_featureset(DkubeFeatureSet(train_fs_name))\n",
    "    api.create_featureset(DkubeFeatureSet(test_fs_name))\n",
    "    api.upload_featurespec(train_fs_name, \"train-fs-spec.yaml\")\n",
    "    api.upload_featurespec(test_fs_name, \"test-fs-spec.yaml\")\n",
    "    #Create model\n",
    "    api.create_model(DkubeModel(username, model_name))\n",
    "except Exception as e:\n",
    "    print(\"ERROR:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='dkube-titanic-pl',\n",
    "    description='example titanic pipeline to submit to leaderboard'\n",
    ")\n",
    "def titanic_pipeline(token, project_id):\n",
    "    preprocessing = dkube_preprocessing_op(token, json.dumps({\"image\": image}),\n",
    "                                            tags=json.dumps([f\"project:{project_id}\"]),\n",
    "                                            program=code_name, run_script=preprocessing_script,\n",
    "                                            datasets=json.dumps([ptrain_dataset, ptest_dataset]), \n",
    "                                            output_featuresets=json.dumps([train_fs_name, test_fs_name]),\n",
    "                                            input_dataset_mounts=json.dumps(dataset_mount_points), \n",
    "                                            output_featureset_mounts=json.dumps(featureset_mount_points))\n",
    "\n",
    "\n",
    "    train       = dkube_training_op(token, json.dumps({\"image\": image}),\n",
    "                                    tags=json.dumps([f\"project:{project_id}\"]),\n",
    "                                    framework=\"tensorflow\", version=\"2.0.0\",\n",
    "                                    program=code_name, run_script=training_script,\n",
    "                                    featuresets=json.dumps([train_fs_name, test_fs_name]), outputs=json.dumps([model_name]),\n",
    "                                    input_featureset_mounts=json.dumps(featureset_mount_points),\n",
    "                                    output_mounts=json.dumps(train_out_mount_points)).after(preprocessing)\n",
    "\n",
    "    predict_op = dkube_job_op(\n",
    "        \"predict\", token, json.dumps({\"image\": image}),\n",
    "        tags=json.dumps([f\"project:{project_id}\"]),\n",
    "        program=code_name, run_script=predict_script,\n",
    "        featuresets=json.dumps([test_fs_name]),input_featureset_mounts=json.dumps([\"/test_fs\"]),\n",
    "        models=json.dumps([model_name]), input_model_mounts=json.dumps([\"/model\"]),\n",
    "        file_outputs={\"output\": \"/output/prediction.csv\", \"mlpipeline-ui-metadata\": \"/output/metrics.json\"},\n",
    "    ).after(train)\n",
    "\n",
    "    predictions = kfp.dsl.InputArgumentPath(predict_op.outputs[\"output\"])\n",
    "    \n",
    "    submit = dkube_submit_op(token, project_id, predictions=predict_op.outputs[\"output\"]).after(predict_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"[{project_name}] Run{runid}\"\n",
    "client.create_run_from_pipeline_func(titanic_pipeline, run_name=run_name, arguments={\"token\":token,\"project_id\":project_id})\n",
    "runid += 1\n",
    "print(f\"RUN NAME:{run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kfp.compiler.Compiler().compile(titanic_pipeline, \"titanic_pipeline.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
