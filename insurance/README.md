# INSURANCE EXAMPLE

- This example supports 3 dataset sources i.e. **Local, AWS S3 and SQL**. 
- By default this example uses local data source.
  - Local data source is created in DKube storage space
- The notebooks in this example can be run inside or outside Dkube.

## Example Flow
- Create DKube resources. This includes Dataset and Model repo resources.
- (Optional) Train a model for Insurance example using SkLearn and deploy the model for inference.
- Create a Modelmonitor. 
  - There is no requirement to deploy a model for this example
  - Generate data for analysis by Modelmonitor
    - Predict data: Inference inputs/outputs
    - Label data:  Dataset with Groundtruth values for the inferences made above
  ** In production, the predict data would be logged by the deployed model and Label data would be generated by experts in the domain manually.
- (Optional) Retrain the model and update Modelmonitor
- Cleanup resources after the example is complete


## Prerequisites
- For Aws_S3 **(S3 bucket is required)**
  - Create an AWS S3 bucket with the name mm-workflow. 
  - You need access and secret keys to access the bucket.
- For SQL **(SQL database is required)**. 
  - You need the following to access the SQL Database
    - username
    - password
    - hostaddress (server address)
    - portnumber
    - databasename


## Section 1: Create Dkube Resources
#### Note: Skip the Dataset (SQL) step if your data is in aws-s3 or in local.It will be automatically created by resources.ipynb notebook.

### Dataset (SQL)
1. Add dataset **insurance-data-sql**
2. Versioning: None
3. Source : SQL
4. Provider : MYSQL
5. Select password and fill the details
- Username, Password, HostAddress, PortNumber, Database Name

### Launch IDE (Inside Dkube)

1. Add Code. Create Code Repo in Dkube with the following information
  - Name: insurance
  - Source: Git
  - URL: https://github.com/oneconvergence/dkube-examples.git
  - Branch : monitoring
2. Create an IDE (JupyterLab)
   - Use sklearn framework
   - **If your data is in local**, move to step 3 directly.
   - **If your data is in aws_s3:**
     - Add the following environment variables with your secret values in configuration tab 
       - AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
   - **If your data is in SQL:**
     - Add the following environment variables with your secret values in configuration tab
       - USERNAME, PASSWORD, HOSTADDRESS, DATABASENAME    
3. Click Submit.
4. From **workspace/insurance/insurance** open resources.ipynb and fill the following details in the first cell. 
     - **MODELMONITOR_NAME** = {your model monitor name}
     - **DATASET_SOURCE** = { one of your choice in ['local' or 'aws_s3' or 'sql'] }
     - **INPUT_TRAIN_TYPE** = {'training'}
     - The following will be derived from the environment automatically. Otherwise, please fill in 
       - **TOKEN** = {your dkube authentication token}
       - **DKUBE_URL** = {your dkube url}
       - **DKUBEUSERNAME** = {your dkube username}
       - If the data source is **aws_s3**, fill the below details also:
         - **ACCESS_KEY** = {your s3 access key}
         - **SECRET_KEY** = {your s3 secret key}
       - If the data source is **sql**, fill the below details also:
         - **HOSTNAME** = {sql server hostname}
         - **DATABASENAME** = {sql database name} 
         - **DBUSERNAME** = {your username}
         - **PASSWORD** = {your password}
5. Run all the cells. This will create all the dkube resources required for this example automatically.

### Launch IDE (Outside Dkube)

1. Download [resources.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance/resources.ipynb)
2. Open the notebook and fill the details in the first cell.
   - **MODELMONITOR_NAME** = {your model monitor name}
   - **DATASET_SOURCE** = { one of your choice in ['local' or 'aws_s3' or 'sql'] }
   - **INPUT_TRAIN_TYPE** = {'training'}
   - **DKUBEUSERNAME** = {your dkube username}
   - **TOKEN** = {your dkube authentication token}
   - **DKUBE_URL** = {your dkube url}
   - If the data source is **aws_s3**, fill the below details also:
     - **ACCESS_KEY** = {your s3 access key}
     - **SECRET_KEY** = {your s3 secret key}
   - If the data source is **sql**, fill the below details also:
     - **HOSTNAME** = {sql server hostname}
     - **DATABASENAME** = {sql database name} 
     - **DBUSERNAME** = {your username}
     - **PASSWORD** = {your password}
3. Run all the cells.

## Section 2: Insurance Model Training (Optional)
** Note: This uses DKube Runs, Kubeflow Pipelines and KfServing. If your DKube configuration doesn't support this, please skip this step and go to Modelmonitoring Section **
1. From **workspace/insurance/insurance** open **train.ipynb** to build the pipeline.
2. The pipeline includes preprocessing, training and serving stages. Run all cells
     - **preprocessing**: the preprocessing stage generates the dataset (either training-data or retraining-data) depending on user choice.
     - **training**: the training stage takes the generated dataset as input, train a sgd model and outputs the model.
     - **serving**: The serving stage takes the generated model and serve it with a predict endpoint for inference. 
3. Verify that the pipeline has created the following resources
     - Datasets: 'insurance-training-data' with version v2.
     - Model: 'insurance-model' with version v2

### Inference
1. Go to webapp directory, and build a docker image with given **Dockerfile** or pull **ocdr/streamlit-webapp:insurance**.
2. Run command  
     - docker run -p 8501:8501 ocdr/streamlit-webapp:insurance 
3. Open http://localhost:8501/ in your browser,
     - Fill serving URL, auth token and other details and click predict.

## Section 3: Modelmonitoring
DKube provides Python SDK for creating a modelmonitor programmatically. You could also choose to create a modelmonitor from the DKube UI. Follow one of the following workflows as per your need.

- **Create Modelmonitor using SDK**
1. From **workspace/insurance/insurance** open [modelmonitor.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance/modelmonitor.ipynb) and run all the cells. New model monitor will be created.
2. Predict and Groundtruth datasets will be generated by Data Generation step.

- **Create Modelmonitor using UI**
  - Follow [README.ui.md](https://github.com/oneconvergence/dkube-examples/blob/monitoring/insurance/README.ui.md) for the next steps.

## Section 4: Data Generation
1. Open [data_generation.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance/data_generation.ipynb) notebook for generating predict and groundtruth datasets.
2. In 1st cell, Update Frequency according to what you set in Modelmonitor. If the d3qatest tag was provided replace it with to use frequency in minutes. For eg: for 5 minutes replace it with `5m` else use `5h` for hours assuming Frequency specified in monitor was 5.
3. Then Run All Cells. It will start Pushing the data, by default it will push the data to local.

## Section 5: Retrain (Optional)
1. Open [resources.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance/resources.ipynb) and set INPUT_TRAIN_TYPE = 'retraining' in the 1st cell and run all the cells.
2. Open train.ipynb and run all the cells.
3. This creates a new version of dataset and a new version of model
   - New dataset version will be created for 'insurance-training-data' dataset
   - New model version will be created for 'insurance-model' model
   - Follow step 4 if you want to retrain via sdk or step 5 if you want to retrain using UI.
4. **SDK**:
   - From **workspace/insurance/insurance** open modelmonitor.ipynb and run the Retraining cell. It will update the dataset and model version in the existing model monitor.
5. **UI**:
   - Edit modelmonitor (UI)
   - Specify the new model version on basic page
   - Specify new dataset version on Training data page
   - Save & Submit
   - Click Next to go to the schema page and Accept the regenerated schema.
   - Wait for a few (30) sec
   - Start the modelmonitor

## Section 6: SMTP Settings (Optional)
Configure your SMTP server settings on Operator screen. This is optional. If SMTP server is not configured, no email alerts will be generated.

## Section 7: Cleanup
1. After your experiment is complete, 
   - Open [resources.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insuranceresources.ipynb) and set CLEANUP=True in last Cleanup cell and run.
   - Open [modelmonitor.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance/modelmonitor.ipynb) and set CLEANUP=True in last Cleanup cell and run.
