# INSURANCE EXAMPLE

- This example only supports predict dataset sources as **CloudEvents**. 

- The notebooks in this example can be run inside or outside Dkube.

## Example Flow
- Create DKube resources. This includes Dataset and Model repo resources.
- Train a model for Insurance example using SkLearn and deploy the model for inference.
- Create a Modelmonitor. 
  - There is a mandatory requirement to deploy a model for this example
- Generate data for analysis by Modelmonitor
  - Predict data: Inference inputs/outputs
  - Label data:  Dataset with Groundtruth values for the inferences made above
  **In production, Label data would be generated by experts in the domain manually.**
- Cleanup resources after the example is complete

## Section 1: Create Dkube Resources

### Launch IDE (Inside Dkube)

#### Note: Follow the instructions if you are running Notebook IDE inside DKube.

1. Add Code. Create Code Repo in Dkube with the following information
  - Name: monitoring-examples
  - Source: Git
  - URL: https://github.com/oneconvergence/dkube-examples.git
  - Branch : **monitoring**
2. Create an IDE (JupyterLab)
   - Use sklearn framework  
3. Click Submit.
4. Open Jupyterlab and from **workspace/insurance/insurance_cloudevents** open [resources.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance_cloudevents/resources.ipynb) and fill the following details in the first cell.
     - **DKUBE_IP** = {IP address of the DKube setup where the prediction deployment is running}
     - **DKUBEUSERNAME** = {your dkube username}
     - **MODELMONITOR_NAME** = {your model monitor name}
     - **MINIO_KEY** = {MINIO access key}, **MINIO_SECRET_KEY** = {MINIO access secret key}
       - MINIO_KEY and MINIO_SECRET_KEY values will be filled automatically by the example with SDK call, these values can also be obtained by running the following commands on the DKube setup where the prediction deployment is running. Provide the creds manually if the user is neither PE nor Operator on the remote cluster.
         - DKube API. Fill in DKUBE_IP and TOKEN in the following curl command
           - `curl -X 'GET' \
                'https://DKUBE_IP:32222/dkube/v2/controller/v2/deployments/logstore' \
                -H 'accept: application/json' \
                -H 'Authorization: Bearer <TOKEN>'`
         - If you have access to Kubernetes, you can get the secrets by running the following commands
           - `kubectl get secret -n dkube-infra cloudevents-minio-secret -o jsonpath="{.data.AWS_ACCESS_KEY_ID}" | base64 -d`
           - `kubectl get secret -n dkube-infra cloudevents-minio-secret -o jsonpath="{.data.AWS_SECRET_ACCESS_KEY}" | base64 -d`
     - The following will be derived from the environment automatically if the notebook is running inside same Dkube IDE. Otherwise in case if the notebook is running locally or in other Dkube Setup , then please fill in, 
       - **TOKEN** = {your dkube authentication token}
       - **DKUBE_URL** = {your dkube url}
     - Modelmonitor run frequency in minutes. The same run interval is used for both Drift & Performance monitoring
         - **RUN_FREQUENCY** = {integer value. units are minutes}
5. Run all the cells. This will create all the dkube resources required for this example automatically.


### Launch IDE (Outside Dkube)

#### Note: Follow the instructions if you are running Notebook IDE outside DKube, for example in VSCode. This is the case with minimal DKube. 

1. Download [resources.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance_cloudevents/resources.ipynb)
2. Open the notebook and fill the details in the first cell.
    - **DKUBE_IP** = {IP address of the DKube where the prediction deployment is running}
    - **DKUBEUSERNAME** = {your dkube username}
    - **MODELMONITOR_NAME** = {your model monitor name}
    - **TOKEN** = {your dkube authentication token}
    - **DKUBE_URL** = {your dkube url}
    - **MINIO_KEY** = {MINIO access key}, **MINIO_SECRET_KEY** = {MINIO access secret key}
       - MINIO_KEY and MINIO_SECRET_KEY values will be filled automatically by the example with SDK call, these values can also be obtained by running the following commands on the DKube setup where the prediction deployment is running. Provide the creds manually if the user is neither PE nor Operator on the remote cluster.
         - DKube API. Fill in DKUBE_IP and TOKEN in the following curl command
           - `curl -X 'GET' \
                'https://DKUBE_IP:32222/dkube/v2/controller/v2/deployments/logstore' \
                -H 'accept: application/json' \
                -H 'Authorization: Bearer <TOKEN>'`
         - If you have access to Kubernetes, you can get the secrets by running the following commands
           - `kubectl get secret -n dkube-infra cloudevents-minio-secret -o jsonpath="{.data.AWS_ACCESS_KEY_ID}" | base64 -d`
           - `kubectl get secret -n dkube-infra cloudevents-minio-secret -o jsonpath="{.data.AWS_SECRET_ACCESS_KEY}" | base64 -d`

3. Run all the cells.

**Note:** In case the monitor is being created on minimal DKube fill the **DKUBEUSERNAME**, **TOKEN**, and **DKUBE_URL** for the minimal DKube. 

## Section 2: Insurance Model Training (Required to deploy model)

#### Note: This uses DKube Runs, Kubeflow Pipelines and KfServing. It requires full Dkube installed. 

1. From **workspace/insurance/insurance_cloudevents** open **train.ipynb** to build the pipeline.
2. The pipeline includes preprocessing, training and serving stages. Run all cells
     - **preprocessing**: the preprocessing stage generates the dataset (either training-data or retraining-data) depending on user choice.
     - **training**: the training stage takes the generated dataset as input, train a sgd model and outputs the model.
     - **serving**: The serving stage takes the generated model and serve it with a predict endpoint for inference.
3. Verify that the pipeline has created the following resources
     - Datasets: 'insurance-training-data' with version v2.
     - Model: 'insurance-model' with version v2

## Section 3: Modelmonitoring
DKube provides Python SDK for creating a modelmonitor programmatically. You could also choose to create a modelmonitor from the DKube UI. This example cuurently support creating monitor using SDK. 

- **Create Modelmonitor using SDK**
1. From **workspace/insurance/insurance_cloudevents** open [modelmonitor.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance_cloudevents/modelmonitor.ipynb) and run all the cells. New model monitor will be created.
2. Predict and Groundtruth dataset data will be generated by Data Generation step and will be utilised by modelmonitor.

## Section 4: Data Generation
1. Open [data_generation.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance_cloudevents/data_generation.ipynb) notebook for making predictions with the deployemnt endpoint and generate groundtruth datasets.
2. In 1st cell, Update Frequency according to what you set in Modelmonitor for Drift. For eg: for 5 minutes, specify it as `5m` and to specify the frequency in hours use `5h` for 5 hours interval.
3. Then Run All Cells. It will start Pushing the data. It uses the data definitions specified in resources.ipynb file.

**Note:** Livedata will be created on the MINIO under deployment id. In the case of minimal dkube, we will create on the remote minio where deployments are running.

## Section 5: SMTP Settings (Optional)
Configure your SMTP server settings on Operator screen. This is optional. If SMTP server is not configured, no email alerts will be generated.

## Section 6: Cleanup
1. After your experiment is complete, 
   - Open [modelmonitor.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance_cloudevents/modelmonitor.ipynb) and set CLEANUP=True in last Cleanup cell and run.
   - Open [resources.ipynb](https://github.com/oneconvergence/dkube-examples/tree/monitoring/insurance_cloudevents/resources.ipynb) and set CLEANUP=True in last Cleanup cell and run.


