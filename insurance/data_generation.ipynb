{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getenv(\"HOME\")\n",
    "if HOME:\n",
    "    EXECUTABLE_DIR = os.path.join(HOME,\".local\", \"bin\")\n",
    "    PATH = os.getenv(\"PATH\")\n",
    "    if EXECUTABLE_DIR not in PATH:\n",
    "        os.environ[\"PATH\"] = f\"{PATH}:{EXECUTABLE_DIR}\"\n",
    "    PATH = os.getenv(\"PATH\")\n",
    "if not os.getenv(\"AWS_BUCKET\"):\n",
    "    os.environ[\"AWS_BUCKET\"] = \"mm-workflow\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install kfp==1.4.0 kfp-server-api==1.2.0 --user &> /dev/null\n",
    "!{sys.executable} -m pip install randomtimestamp --user\n",
    "!{sys.executable} -m pip install pymysql --user\n",
    "#!{sys.executable} -m pip install --upgrade pyodbc --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HOME:\n",
    "    USR_LOCAL_LIB_PATH = os.path.join(HOME,\".local\",\"lib\",\"python3.6\",\"site-packages\")\n",
    "    if USR_LOCAL_LIB_PATH not in sys.path:\n",
    "        sys.path.append(USR_LOCAL_LIB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kfp import components\n",
    "from kubernetes import client as k8s_client\n",
    "import os\n",
    "import json\n",
    "import kfp\n",
    "import requests\n",
    "import string\n",
    "import random\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kfp import components\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "from dkube.sdk.api import DkubeApi\n",
    "from dkube.sdk.rsrcs import DkubeCode\n",
    "from dkube.sdk.rsrcs import DkubeDataset\n",
    "from dkube.sdk.rsrcs import DkubeModel\n",
    "\n",
    "\n",
    "## Dependencies for data generator \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from configparser import ConfigParser\n",
    "import numpy.random,argparse,uuid\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import boto3\n",
    "import time\n",
    "import joblib\n",
    "from randomtimestamp import randomtimestamp\n",
    "from sklearn import preprocessing as skpreprocessing\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import io\n",
    "import re\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "from joblib import dump, load\n",
    "from sqlalchemy import create_engine\n",
    "#import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBCONFIG:\n",
    "    def __init__(self, hostname, databasename, username, password):\n",
    "        self.hostname = hostname\n",
    "        self.databasename = databasename\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"mysql+pymysql://{self.username}:{self.password}@{self.hostname}/{self.databasename}\"\n",
    "    def __repr__(self):\n",
    "        return f\"mysql+pymysql://{self.username}:{self.password}@{self.hostname}/{self.databasename}\"\n",
    "\n",
    "class DataSource(Enum):\n",
    "    LOCAL = \"local\"\n",
    "    AWS_S3 = \"aws_s3\"\n",
    "    SQL = \"sql\"\n",
    "    \n",
    "\n",
    "DatasetSource = namedtuple('DatasetSource', 'model_monitor table frequency_unit data_class add_prefix_ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE_DATA_S3_PATH = \"insurance-base/insurance.csv\"\n",
    "MONITOR_NAME =\"mm-demo\"\n",
    "FREQUENCY = \"5m\"\n",
    "MODEL_FREQUENCY = 4\n",
    "\n",
    "DATASET_SAMPLES  = 6\n",
    "\n",
    "PREDICT_DATASET_TABLE   = \"insurance_predict\" \n",
    "LABELLED_DATASET_TABLE  = \"insurance_gt\"\n",
    "\n",
    "PREDICT_DATA_CLASS = \"predict\" # used for s3 \n",
    "LABELLED_DATA_CLASS = \"groundtruth\" #used for s3\n",
    "\n",
    "PREFIX_PREDICT_DATASET_WITH_TS = True\n",
    "PREFIX_LABELLED_DATASET_WITH_TS = False\n",
    "\n",
    "DATASET_SOURCE = DataSource.AWS_S3\n",
    "\n",
    "HOSTNAME = \"\"\n",
    "DATABASE_NAME = \"\"\n",
    "USERNAME = \"\"\n",
    "PASSWORD = \"\"\n",
    "\n",
    "# MODEL FOR PREDICTION\n",
    "train_model = load('model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_password():\n",
    "    datum_name = \"sql-data\"\n",
    "    user = os.getenv(\"DKUBE_USER_LOGIN_NAME\")\n",
    "    headers={\"authorization\": \"Bearer \"+os.getenv(\"DKUBE_USER_ACCESS_TOKEN\")}\n",
    "    url = \"http://dkube-controller-worker.dkube:5000/dkube/v2/controller/users/%s/datums/class/dataset/datum/%s\"\n",
    "    resp = requests.get(url % (user, datum_name), headers=headers).json()\n",
    "    return resp['data']['datum']['sql']['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsuranceDataGenerator:\n",
    "    # With no parameters or configuration, boto3 will look for\n",
    "    # access keys in these places:\n",
    "    # 1. Environment variables (AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY)\n",
    "    # 2. Credentials file (~/.aws/credentials or\n",
    "    #      C:\\Users\\USER_NAME\\.aws\\credentials)\n",
    "    # 3. AWS IAM role for Amazon EC2 instance\n",
    "    #    (http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html)\n",
    "\n",
    "    #    Define a ~/.aws/credentials file as following\n",
    "    #    [default]\n",
    "    #    aws_access_key_id=foo\n",
    "    #    aws_secret_access_key=bar\n",
    "    #    aws_session_token=baz # might not be required\n",
    "    BUCKET = None\n",
    "    S3_CLIENT = None\n",
    "    DB_ENGINE = None\n",
    "    API_CLIENT = None\n",
    "    TOKEN = None\n",
    "    USERNAME = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_reference_s3path,\n",
    "        monitor_name,\n",
    "        n_predict_datasets: int = 1,\n",
    "        n_groundtruth_datasets: int = 1,\n",
    "        n_drift_datasets: int = 1,\n",
    "        start_time: datetime.datetime = None,\n",
    "        frequency=\"1H\",\n",
    "        model_frequency=10,\n",
    "        duration: str = \"10:24:12\",\n",
    "        margin=20,\n",
    "        db_config:DBCONFIG = None,\n",
    "        dataset_source: DataSource = DataSource.AWS_S3\n",
    "    ):\n",
    "        if not re.fullmatch(\"^\\d+[hmHM]{1}$\",frequency):\n",
    "            raise ValueError(\"frequency can have [time_value_int][time_unit] time_unit can be case case insensitive out of H, M\")\n",
    "        self.n_predict_datasets = n_predict_datasets\n",
    "        self.frequency  = frequency\n",
    "        self.margin=margin\n",
    "        self.monitor_name = monitor_name\n",
    "        self.data_reference_s3path = data_reference_s3path\n",
    "        self.__reference_df = None\n",
    "        self.n_groundtruth_datasets = n_drift_datasets\n",
    "        self.n_drift_datasets = n_drift_datasets\n",
    "        self.dataset_source = dataset_source\n",
    "        self.start_time = start_time if start_time else datetime.datetime.utcnow()\n",
    "        self.drift_seeds = [572, 1968, 2254 ,2642 , 2864, 3164]\n",
    "        self.input_features = ['age', 'sex', 'bmi', 'children', 'smoker', 'region']\n",
    "        self.model_frequency = model_frequency\n",
    "        self.predict_start = datetime.datetime.utcnow()\n",
    "        self.drift_start = datetime.datetime.utcnow()\n",
    "        self.train_data = None\n",
    "        self.sex_values = None\n",
    "        self.children_values = None\n",
    "        self.region_values = None\n",
    "        self.age_min = self.age_max = None\n",
    "        self.bmi_min = self.bmi_max = None\n",
    "        self.model = None\n",
    "        self.init_train_data()\n",
    "        self.load_model()\n",
    "        \n",
    "        if n_groundtruth_datasets > n_predict_datasets or n_drift_datasets > n_predict_datasets:\n",
    "            raise Exception(\"GroundTruth datasets or drift_datsets cant be greater than predict datasets\")\n",
    "        self.db_config = db_config\n",
    "            \n",
    "        self.duration = duration\n",
    "        klass = type(self)\n",
    "        if not klass.BUCKET:\n",
    "            klass.BUCKET = os.getenv(\"AWS_BUCKET\")\n",
    "        if not klass.S3_CLIENT:\n",
    "            klass.S3_CLIENT = boto3.client(\"s3\")\n",
    "        if not klass.TOKEN:\n",
    "            klass.TOKEN = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\")\n",
    "        if not klass.USERNAME:\n",
    "            klass.USERNAME= os.getenv(\"USERNAME\")\n",
    "        if not klass.API_CLIENT:\n",
    "            klass.API_CLIENT = DkubeApi(token=klass.TOKEN)\n",
    "        if not klass.DB_ENGINE:\n",
    "            if self.db_config:\n",
    "                klass.DB_ENGINE = create_engine(str(self.db_config))\n",
    "\n",
    "        duration = self.duration.split(\"-\")\n",
    "        if len(duration) < 2:\n",
    "            duration.append(\"0\")\n",
    "            duration.append(\"0\")\n",
    "        elif len(duration) < 3:\n",
    "            duration.append(\"0\")\n",
    "    \n",
    "    def init_train_data(self):\n",
    "        insurance = pd.read_csv(\"https://storage.googleapis.com/insurance-data/insurance/insurance.csv\")\n",
    "        for col in ['sex', 'smoker', 'region']:\n",
    "            if (insurance[col].dtype == 'object'):\n",
    "                le = preprocessing.LabelEncoder()\n",
    "                le = le.fit(insurance[col])\n",
    "                insurance[col] = le.transform(insurance[col])\n",
    "        self.train_data = insurance\n",
    "        self.sex_values = insurance[\"sex\"].unique().tolist()\n",
    "        self.children_values = insurance[\"children\"].unique().tolist()\n",
    "        self.smoker_values = insurance[\"smoker\"].unique().tolist()\n",
    "        self.region_values = insurance[\"region\"].unique().tolist()\n",
    "        self.age_min, self.age_max = insurance[\"age\"].min(), insurance[\"age\"].max()\n",
    "        self.bmi_min, self.bmi_max = insurance[\"bmi\"].min(), insurance[\"bmi\"].max()\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.model = joblib.load(\"model.joblib\")\n",
    "    \n",
    "    @classmethod\n",
    "    def save_dataset_to_s3(cls, data, monitor_name, name, typeofdata, prefix_dir_with_ts = True, frequency_unit=\"H\",current_date=None):\n",
    "        file_name = name + \".csv\"\n",
    "        if not current_date:\n",
    "            current_date = datetime.datetime.now()\n",
    "        data_dir = os.path.join(\n",
    "            monitor_name,\n",
    "            typeofdata\n",
    "        )\n",
    "        if prefix_dir_with_ts:\n",
    "            data_dir = os.path.join(data_dir, \n",
    "            current_date.strftime(\"%Y\"),\n",
    "            current_date.strftime(\"%m\"),\n",
    "            current_date.strftime(\"%d\"),\n",
    "            current_date.strftime(\"%H\"))\n",
    "            if frequency_unit.lower() ==\"m\":\n",
    "                data_dir = os.path.join(data_dir,current_date.strftime(\"%M\"))\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        with io.StringIO() as csv_buffer:\n",
    "            data.to_csv(csv_buffer, index=False)\n",
    "\n",
    "            response = cls.S3_CLIENT.put_object(\n",
    "                Bucket=cls.BUCKET, Key=file_path, Body=csv_buffer.getvalue()\n",
    "            )\n",
    "            status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "            if status == 200:\n",
    "                print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "                return file_path\n",
    "            else:\n",
    "                print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n",
    "                \n",
    "    @staticmethod\n",
    "    def save_dataset_to_local(data, name, monitor_name,typeofdata, current_date=None):\n",
    "        file_name = name + \".csv\"\n",
    "        try:\n",
    "            data_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "        except:\n",
    "            data_dir = os.getcwd()\n",
    "        if not current_date:\n",
    "            current_date = datetime.datetime.now()\n",
    "        data_dir = os.path.join(\n",
    "            data_dir,\n",
    "            monitor_name,\n",
    "            typeofdata,\n",
    "            current_date.strftime(\"%Y\"),\n",
    "            current_date.strftime(\"%m\"),\n",
    "            current_date.strftime(\"%d\"),\n",
    "            current_date.strftime(\"%H\"),\n",
    "        )\n",
    "        if not os.path.isdir(data_dir):\n",
    "            os.makedirs(data_dir, exists_ok=True)\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        data.to_csv(file_path, index=False)       \n",
    "        return file_path\n",
    "    \n",
    "    @classmethod\n",
    "    def save_dataset_to_sql(cls, data, tablename):\n",
    "        data.to_sql(tablename, cls.DB_ENGINE, if_exists=\"append\", index=False)\n",
    "    \n",
    "    def save_dataset(self ,data, data_name:str, config: DatasetSource, current_date=None):\n",
    "        klass = type(self)\n",
    "        if self.dataset_source == DataSource.AWS_S3:\n",
    "            return klass.save_dataset_to_s3(data, config.model_monitor, data_name, config.data_class, config.add_prefix_ts, config.frequency_unit, current_date)\n",
    "        elif self.dataset_source == DataSource.SQL:\n",
    "            klass.save_dataset_to_sql(data, config.table)\n",
    "        elif self.dataset_source == DataSource.LOCAL:\n",
    "            return klass.save_dataset_to_local(data, data_name, config.model_monitor, config.data_class, current_date)\n",
    "\n",
    "    @property\n",
    "    def frequency_ts(self):\n",
    "        value = int(self.frequency[:-1])\n",
    "        unit = self.frequency[-1].lower()\n",
    "        seconds_per_unit = {\"s\": 1, \"m\": 60, \"h\": 3600, \"d\": 86400, \"w\": 604800}\n",
    "        seconds_count = int(value) * seconds_per_unit[unit]\n",
    "        now = datetime.datetime.utcnow()\n",
    "        if unit.lower() == \"h\":\n",
    "            delta = datetime.timedelta(hours=value)\n",
    "            new_time = (now+delta).replace(minute = 0, second =0, microsecond=0) - datetime.timedelta(seconds=self.margin)\n",
    "            second_remaining = (new_time-now).seconds\n",
    "            result =  seconds_count if second_remaining > seconds_count or second_remaining == 0 else second_remaining\n",
    "            print(f\"Next Push after {datetime.timedelta(seconds=result)}\")\n",
    "            return result        \n",
    "        elif unit == \"m\":\n",
    "            diff = abs(now.minute%-value)\n",
    "            if diff == 0:\n",
    "                delta = datetime.timedelta(minutes=value)\n",
    "                new_time = (now+delta).replace(second =0, microsecond=0) - datetime.timedelta(seconds=self.margin)\n",
    "                result = (new_time-now).seconds\n",
    "                print(f\"Next Push after {datetime.timedelta(seconds=result)}\")\n",
    "                return result\n",
    "            else:\n",
    "                delta = datetime.timedelta(minutes = diff)\n",
    "                new_time = (now+delta).replace(second =0, microsecond=0) - datetime.timedelta(seconds=self.margin)\n",
    "                second_remaining = (new_time-now).seconds\n",
    "                result =  seconds_count if second_remaining > seconds_count or second_remaining == 0 else second_remaining\n",
    "                print(f\"Next Push after {datetime.timedelta(seconds=result)}\")\n",
    "                return result\n",
    "        \n",
    "\n",
    "    \n",
    "    @property\n",
    "    def awsS3Secret(self):\n",
    "        AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY_ID\",\"\") \n",
    "        AWS_SECRET_KEY = os.getenv(\"AWS_ACCESS_SECRET_KEY\",\"\")\n",
    "        if AWS_ACCESS_KEY and AWS_SECRET_KEY:\n",
    "            return {\"access_key\":AWS_ACCESS_KEY, \"secret_key\": AWS_SECRET_KEY}\n",
    "        else:\n",
    "            home_dir = os.getenv(\"HOME\")\n",
    "            if home_dir:\n",
    "                creds_path = os.path.join(home_dir, \".aws\",\"credentials\")\n",
    "                config = ConfigParser()\n",
    "                if os.path.isfile(creds_path):\n",
    "                    config.read(creds_path)\n",
    "                    if \"default\" in config:\n",
    "                        AWS_ACCESS_KEY = config[\"default\"][\"aws_access_key_id\"]\n",
    "                        AWS_SECRET_KEY = config[\"default\"][\"aws_secret_access_key\"]\n",
    "                        if AWS_ACCESS_KEY and AWS_SECRET_KEY:\n",
    "                            return {\"access_key\":AWS_ACCESS_KEY, \"secret_key\": AWS_SECRET_KEY}\n",
    "                \n",
    "    def create_aws_dkube_dataset(self, path,ds_class):\n",
    "        klass = type(self)\n",
    "        try:\n",
    "            ds = klass.API_CLIENT.get_repo(\"dataset\",klass.USERNAME,f\"{self.monitor_name}-{ds_class}\")\n",
    "        except Exception  as e:\n",
    "            ds = DkubeDataset(klass.USERNAME, f\"{self.monitor_name}-{ds_class}\", remote=True)\n",
    "            ds.update_dataset_source('aws_s3')\n",
    "            secret = self.awsS3Secret\n",
    "            if secret:\n",
    "                ds.update_awss3_details(klass.BUCKET,path, secret[\"access_key\"],secret[\"secret_key\"])\n",
    "                klass.API_CLIENT.create_dataset(ds)\n",
    "        else:\n",
    "            print(f\"{self.monitor_name}-{ds_class} dataset already existing\")\n",
    "        \n",
    "    @property\n",
    "    def end(self):\n",
    "        duration = self.duration.split(\":\")\n",
    "        if len(duration) < 2:\n",
    "            duration.append(\"0\")\n",
    "            duration.append(\"0\")\n",
    "        elif len(duration) < 3:\n",
    "            duration.append(\"0\")\n",
    "        return self.start_time + datetime.timedelta(\n",
    "            hours=int(duration[0]), minutes=int(duration[1]), seconds=int(duration[2])\n",
    "        )\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def reference_df(self):\n",
    "        if self.__reference_df == None:\n",
    "            self.__reference_df = self.get_df_from_s3(self.data_reference_s3path)\n",
    "        return self.__reference_df\n",
    "\n",
    "    @classmethod\n",
    "    def get_df_from_s3(cls, path):\n",
    "        response = cls.S3_CLIENT.get_object(Bucket=cls.BUCKET, Key=path)\n",
    "        status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "        if status == 200:\n",
    "            print(f\"Successful S3 get_object response. Status - {status}\")\n",
    "            data = pd.read_csv(response.get(\"Body\"))\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"Unsuccessful S3 get_object response. Status - {status}\")\n",
    "\n",
    "\n",
    "\n",
    "    @property\n",
    "    def train_df(self):\n",
    "        data = self.reference_df\n",
    "        train = data.drop([\"charges\"], axis=1)\n",
    "        y = data[\"charges\"]\n",
    "        a = np.arange(0, train.shape[1])\n",
    "        train_aug = pd.DataFrame(\n",
    "            index=train.index, columns=train.columns, dtype=\"float64\"\n",
    "        )\n",
    "\n",
    "        for i in tqdm(range(0, len(train))):\n",
    "            AUG_FEATURE_RATIO = 0.5\n",
    "            AUG_FEATURE_COUNT = np.floor(train.shape[1] * AUG_FEATURE_RATIO).astype(\n",
    "                \"int16\"\n",
    "            )\n",
    "\n",
    "            aug_feature_index = np.random.choice(\n",
    "                train.shape[1], AUG_FEATURE_COUNT, replace=False\n",
    "            )\n",
    "            aug_feature_index.sort()\n",
    "\n",
    "            feature_index = np.where(np.logical_not(np.in1d(a, aug_feature_index)))[0]\n",
    "\n",
    "            train_aug.iloc[i, feature_index] = train.iloc[i, feature_index]\n",
    "\n",
    "            rand_row_index = np.random.choice(\n",
    "                len(train), len(aug_feature_index), replace=True\n",
    "            )\n",
    "\n",
    "            for n, j in enumerate(aug_feature_index):\n",
    "                train_aug.iloc[i, j] = train.iloc[rand_row_index[n], j]\n",
    "\n",
    "        train_aug[\"charges\"] = y + y * 0.03\n",
    "        train_all = pd.concat([data, train_aug])\n",
    "        return train_all\n",
    "    \n",
    "    \n",
    "    def train_test_split(self, test_size = 0.1):\n",
    "        train_all = self.train_df\n",
    "        train_dataset, predict_data = train_test_split(train_all, test_size=test_size,random_state=self.n_predict_datasets)\n",
    "        train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "        for i in range(0, len(train_dataset)):\n",
    "            train_dataset.loc[i, \"unique_id\"] = uuid.uuid4()\n",
    "\n",
    "        for dataframe in [train_dataset, predict_data]:\n",
    "            for col in [\"sex\", \"smoker\", \"region\"]:\n",
    "                if dataframe[col].dtype == \"object\":\n",
    "                    le = skpreprocessing.LabelEncoder()\n",
    "                    le = le.fit(dataframe[col])\n",
    "                    dataframe[col] = le.transform(dataframe[col])\n",
    "                    print(\"Completed Label encoding on\", col)\n",
    "\n",
    "        predict_data = predict_data.reset_index(drop=True)\n",
    "        for i in range(0, len(predict_data)):\n",
    "            predict_data.loc[i, \"unique_id\"] = uuid.uuid4()\n",
    "        return train_dataset, predict_data\n",
    "    \n",
    "    def sample_predict_data(self, predict_data = None,train_test_split=0.1):\n",
    "        if predict_data is None:\n",
    "            _,predict_data = self.train_test_split(train_test_split)\n",
    "        n_predict_rows = predict_data.shape[0] // self.n_predict_datasets\n",
    "        index = 0\n",
    "        \n",
    "        input_features = [\"age\",\"sex\",\"bmi\",\"region\",\"children\",\"smoker\"]\n",
    "        predict_data['charges'] = train_model.predict(predict_data[input_features])\n",
    "        for i in range(1, self.n_predict_datasets + 1):\n",
    "            pred_data = predict_data.iloc[index : index + n_predict_rows, :]\n",
    "            pred_data_name = str(i) + \"_predict_data\"\n",
    "            index += n_predict_rows\n",
    "            yield {\"name\": pred_data_name, \"df\": pred_data}\n",
    "\n",
    "    \n",
    "    def generate_groundtruth_samples(self, predict_data = None,train_test_split=0.1):\n",
    "        inp_features = ['age','sex','bmi','region','children','smoker']\n",
    "        for i, data in enumerate(self.sample_predict_data(predict_data, train_test_split)):\n",
    "            gt_data = data[\"df\"]\n",
    "            gt_data[\"GT_target\"] = gt_data[\"charges\"] + gt_data[\"charges\"] * 0.05\n",
    "            gt_data = gt_data.drop([\"charges\"], axis=1)\n",
    "            gt_name = str(i+1) + \"_GTpredict_data\"\n",
    "            if i > self.n_groundtruth_datasets:\n",
    "                return\n",
    "            yield {\"name\": gt_name, \"df\": gt_data}\n",
    "\n",
    "    def generate_gt_samples(self,predict_data):\n",
    "        for i, data in enumerate(predict_data):\n",
    "            gt_data = data[\"df\"]\n",
    "            gt_data[\"GT_target\"] = gt_data[\"charges\"] + gt_data[\"charges\"] * 0.05\n",
    "            gt_data = gt_data.drop([\"charges\"], axis=1)\n",
    "            gt_name = str(i+1) + \"_GTpredict_data\"\n",
    "            if i > self.n_groundtruth_datasets:\n",
    "                return\n",
    "            yield {\"name\": gt_name, \"df\": gt_data}\n",
    "            \n",
    "\n",
    "    def generate_all_predict(self): # All features will not have drift\n",
    "\n",
    "        inp_features = ['age','sex','bmi','region','children','smoker']\n",
    "        \n",
    "        for i in range(1, self.n_predict_datasets + 1):\n",
    "            no_of_samples = np.random.randint(80,100)\n",
    "            predict_df = self.train_data[self.input_features].sample(no_of_samples)\n",
    "            predict_df[\"charges\"] = self.model.predict(predict_df[self.input_features])\n",
    "            start = self.predict_start\n",
    "            end = start + datetime.timedelta(minutes=self.model_frequency)\n",
    "            predict_df[\"uuid\"] = [str(uuid.uuid4()) for i in range(no_of_samples)]\n",
    "            predict_df[\"timestamp\"] = pd.date_range(start, end, no_of_samples)\n",
    "            pred_data_name = str(i) + \"_predict_data\"\n",
    "            yield {\"name\": pred_data_name, \"df\": predict_df}\n",
    "    \n",
    "\n",
    "    def generate_all_drift(self): ## All features will have drift\n",
    "        for i in range(1,self.n_drift_datasets):\n",
    "            state = np.random.get_state()\n",
    "            seed = np.random.choice(self.drift_seeds)\n",
    "            np.random.seed(seed)\n",
    "            start = self.drift_start\n",
    "            end = start + datetime.timedelta(minutes=self.model_frequency)\n",
    "            no_of_samples = np.random.randint(80,100)\n",
    "            drift_df = pd.DataFrame({\n",
    "                'age' : np.random.randint(self.age_max-15,self.age_max,no_of_samples).tolist(),\n",
    "                'sex' : np.repeat([np.random.choice(self.sex_values)], no_of_samples).tolist(),\n",
    "                'bmi' : np.random.uniform(self.bmi_min, self.bmi_max,no_of_samples).tolist(),\n",
    "                'children' : np.random.choice(self.children_values, no_of_samples).tolist(),\n",
    "                'smoker' : np.random.choice(self.smoker_values, no_of_samples).tolist(),\n",
    "                'region' : np.random.choice(self.region_values, no_of_samples).tolist(),\n",
    "                'uuid' : [str(uuid.uuid4()) for i in range(no_of_samples)],\n",
    "                'timestamp': pd.date_range(start, end, no_of_samples)\n",
    "            })\n",
    "            drift_df[\"charges\"] = self.model.predict(drift_df[self.input_features])\n",
    "            np.random.set_state(state)\n",
    "            drift_data_name = str(i) + \"_drifted_data\"\n",
    "            yield {\"name\": drift_data_name, \"df\": drift_df}\n",
    "            \n",
    "    \n",
    "    \n",
    "    def generate_random_drift(self): # Some features will have drift\n",
    "        for i in range(1,self.n_drift_datasets):\n",
    "            start = self.predict_start\n",
    "            end = start + datetime.timedelta(minutes=self.model_frequency)\n",
    "            no_of_samples = np.random.randint(80,100)\n",
    "            predict_df = pd.DataFrame({\n",
    "                'age' : np.random.randint(self.age_min,self.age_max,no_of_samples).tolist(),\n",
    "                'sex' : np.random.choice(self.sex_values, no_of_samples).tolist(),\n",
    "                'bmi' : np.random.uniform(self.bmi_min, self.bmi_max,no_of_samples).tolist(),\n",
    "                'children' : np.random.choice(self.children_values[:4], no_of_samples).tolist(),\n",
    "                'smoker' : np.random.choice(self.smoker_values, no_of_samples).tolist(),\n",
    "                'region' : np.random.choice(self.region_values, no_of_samples).tolist(),\n",
    "                'uuid' : [str(uuid.uuid4()) for i in range(no_of_samples)],\n",
    "                'timestamp': pd.date_range(start, end, no_of_samples)\n",
    "            })\n",
    "            predict_df[\"charges\"] = self.model.predict(predict_df[self.input_features])\n",
    "            drift_data_name = str(i) + \"_drifted_data\"\n",
    "            yield {\"name\": drift_data_name, \"df\": predict_df}   \n",
    "    \n",
    "    def generate_drift_datasets(self, predict_data = None,train_test_split=0.1):\n",
    "        for j, data in enumerate(self.sample_predict_data(predict_data,train_test_split)):\n",
    "            drifted_data = data[\"df\"]\n",
    "            if j % 2 == 0:\n",
    "                for i in range(0, len(drifted_data)):\n",
    "                    rndm = random.randint(0,2)\n",
    "                    if rndm == 0:\n",
    "                        drifted_data[\"age\"].iloc[i] = drifted_data[\"age\"].iloc[i] + random.randint(15,80)\n",
    "                    elif rndm == 1:\n",
    "                        drifted_data[\"bmi\"].iloc[i] = drifted_data[\"bmi\"].iloc[i] + random.randint(15,50)\n",
    "                    else:\n",
    "                        drifted_data[\"age\"].iloc[i] = drifted_data[\"age\"].iloc[i] + random.randint(15,80)\n",
    "                        drifted_data[\"bmi\"].iloc[i] = drifted_data[\"bmi\"].iloc[i] + random.randint(15,50)\n",
    "            else:\n",
    "                random_rows_count = random.randint(0, len(drifted_data)-1)\n",
    "                region = [\"southeast\", \"northwest\"]\n",
    "                sex = [\"male\",\"female\"]\n",
    "                for i in range(random_rows_count):\n",
    "                    random_index = random.randint(0,len(drifted_data)-1)\n",
    "                    random_gender_idx = random.randint(0,1)\n",
    "                    drifted_data[\"sex\"] = sex[random_gender_idx]\n",
    "                    drifted_data[\"sex\"].iloc[random_index] = sex[0 if random_gender_idx else 0]\n",
    "                    random_index = random.randint(0,len(drifted_data)-1)\n",
    "                    random_region_index = random.randint(0,1)\n",
    "                    drifted_data[\"region\"] = region[random_region_index]\n",
    "                    drifted_data[\"region\"].iloc[random_index] = region[0 if random_gender_idx else 0]\n",
    "            drifted_name = str(j+1) + \"_drifted_data\"\n",
    "            if j > self.n_drift_datasets:\n",
    "                return\n",
    "            yield {\"name\": drifted_name, \"df\": drifted_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = InsuranceDataGenerator(REFERENCE_DATA_S3_PATH,\n",
    "                                   MONITOR_NAME,\n",
    "                                   DATASET_SAMPLES,\n",
    "                                   DATASET_SAMPLES,\n",
    "                                   DATASET_SAMPLES,\n",
    "                                   frequency=FREQUENCY,\n",
    "                                   model_frequency = MODEL_FREQUENCY,\n",
    "                                   db_config = DBCONFIG(\n",
    "                                       hostname=HOSTNAME,\n",
    "                                       databasename = DATABASE_NAME,\n",
    "                                       username = USERNAME,\n",
    "                                       password = PASSWORD),\n",
    "                                   dataset_source = DATASET_SOURCE)\n",
    "\n",
    "predict_dataset_source = DatasetSource(model_monitor=MONITOR_NAME,\n",
    "                                       table=PREDICT_DATASET_TABLE, \n",
    "                                       frequency_unit = generator.frequency[-1],\n",
    "                                       data_class=PREDICT_DATA_CLASS,\n",
    "                                       add_prefix_ts=PREFIX_PREDICT_DATASET_WITH_TS)\n",
    "ground_dataset_source = DatasetSource(model_monitor=MONITOR_NAME,\n",
    "                                      table=LABELLED_DATASET_TABLE,\n",
    "                                      data_class=LABELLED_DATA_CLASS,\n",
    "                                      frequency_unit = generator.frequency[-1],\n",
    "                                      add_prefix_ts=PREFIX_LABELLED_DATASET_WITH_TS )\n",
    "\n",
    "train_predict = generator.train_test_split()\n",
    "trainds, testds = train_predict\n",
    "#predict_samples = list(generator.sample_predict_data(testds))\n",
    "#groundtruth_samples= list(generator.generate_groundtruth_samples(testds))\n",
    "#drift_datasets = list(generator.generate_drift_datasets(testds))\n",
    "\n",
    "# prediction data with no drift\n",
    "predict_samples = list(generator.generate_all_predict())\n",
    "groundtruth_samples= list(generator.generate_gt_samples(predict_samples))\n",
    "\n",
    "## Random drift or all drift\n",
    "drift_datasets = list(np.random.choice([generator.generate_random_drift(), generator.generate_all_drift()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Training Data\n",
    "# training_path = InsuranceDataGenerator.save_dataset_to_s3(trainds, generator.monitor_name,\"training\",\"training\",False)\n",
    "# generator.create_aws_dkube_dataset(training_path,\"training\")\n",
    "drift_path = []\n",
    "predict_path = []\n",
    "groundtruth_path = []\n",
    "for i, data in enumerate(predict_samples):\n",
    "    second_remaining = generator.frequency_ts\n",
    "    time.sleep(second_remaining)\n",
    "    pushed_date = datetime.datetime.utcnow()\n",
    "    sample_count = round(random.uniform(0.5 ,0.9),2)\n",
    "    if i%2:\n",
    "        p_ts = [\n",
    "            randomtimestamp(start=pushed_date-datetime.timedelta(seconds=second_remaining),\n",
    "                            end=pushed_date,pattern = \"'%Y-%m-%d %H:%M:%S.%f'\") \n",
    "            for i in range(len(data[\"df\"]))\n",
    "            ]\n",
    "        data[\"df\"][\"timestamp\"] = p_ts\n",
    "        p_path = generator.save_dataset(data[\"df\"].sample(frac=sample_count),data[\"name\"],predict_dataset_source)\n",
    "        if len(predict_path) == 0:\n",
    "            generator.create_aws_dkube_dataset(os.path.join(f\"{generator.monitor_name}/{predict_dataset_source.data_class}\"),predict_dataset_source.data_class)\n",
    "        if p_path:\n",
    "            predict_path.append(p_path)\n",
    "    else:\n",
    "        p_ts = [randomtimestamp(\n",
    "                start=pushed_date-datetime.timedelta(seconds=second_remaining), end=pushed_date,pattern = \"'%Y-%m-%d %H:%M:%S.%f'\"\n",
    "            ) for i in range(len(drift_datasets[i][\"df\"]))]\n",
    "        drift_datasets[i][\"df\"][\"timestamp\"] = p_ts\n",
    "        d_path = generator.save_dataset(drift_datasets[i][\"df\"].sample(frac=sample_count), drift_datasets[i][\"name\"],predict_dataset_source,pushed_date)\n",
    "        if len(drift_path) == 0:\n",
    "            generator.create_aws_dkube_dataset(os.path.join(f\"{generator.monitor_name}/{predict_dataset_source.data_class}\"),predict_dataset_source.data_class)\n",
    "        if d_path:\n",
    "            drift_path.append(d_path)\n",
    "    groundtruth_samples[i][\"df\"][\"timestamp\"] = p_ts[0:len(groundtruth_samples[i][\"df\"])]\n",
    "    g_path = generator.save_dataset(groundtruth_samples[i][\"df\"].sample(frac=sample_count), groundtruth_samples[i][\"name\"],ground_dataset_source, pushed_date)\n",
    "    if len(groundtruth_path) == 0:\n",
    "        generator.create_aws_dkube_dataset(os.path.join(f\"{generator.monitor_name}/{ground_dataset_source.data_class}\"),ground_dataset_source.data_class)\n",
    "    if g_path:\n",
    "        groundtruth_path.append(g_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
