{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "import sys,json, os\n",
    "job_class = os.getenv(\"DKUBE_JOB_CLASS\")\n",
    "if not job_class:\n",
    "    !{sys.executable} -m pip install kfp==1.4.0 kfp-server-api==1.2.0 --user >/dev/null\n",
    "    !{sys.executable} -m pip install git+https://github.com/oneconvergence/dkube.git@3.3 --upgrade --user >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kfp import components\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "import os\n",
    "import kfp\n",
    "import string\n",
    "import random\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kfp import components\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "from dkube.sdk.api import DkubeApi\n",
    "from dkube.sdk.rsrcs import DkubeCode\n",
    "from dkube.sdk.rsrcs import DkubeDataset\n",
    "from dkube.sdk.rsrcs import DkubeModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dkube_preprocessing_op = components.load_component_from_file(\"/mnt/dkube/pipeline/components/preprocess/component.yaml\")\n",
    "dkube_training_op = components.load_component_from_file(\"/mnt/dkube/pipeline/components/training/component.yaml\")\n",
    "dkube_serving_op  = components.load_component_from_file(\"/mnt/dkube/pipeline/components/serving/component.yaml\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = DkubeApi(token=os.getenv(\"DKUBE_USER_ACCESS_TOKEN\", d3_config[\"TOKEN\"]))\n",
    "client = kfp.Client(\n",
    "    host=os.getenv(\"KF_PIPELINES_ENDPOINT\"),\n",
    "    existing_token=os.getenv(\"DKUBE_USER_ACCESS_TOKEN\", d3_config[\"TOKEN\"]),\n",
    "    namespace=d3_config['DKUBEUSERNAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_program = d3_config[\"DKUBE_TRAINING_CODE_NAME\"]\n",
    "if (d3_config[\"DATA_SOURCE\"] == 'local' or d3_config[\"DATA_SOURCE\"] == 'aws_s3') and d3_config[\"INPUT_TRAIN_TYPE\"] == 'retraining':\n",
    "    input_training_dataset = d3_config['MONITOR_NAME'] +'-groundtruth'\n",
    "else:\n",
    "    input_training_dataset = d3_config['DKUBE_BASE_DATASET']\n",
    "\n",
    "\n",
    "## Preprocessing stage inputs\n",
    "preprocessing_script =f\"pip3 install pymysql --user;python insurance_datasources/preprocessing.py --data_source {d3_config['DATA_SOURCE']} --train_type {d3_config['INPUT_TRAIN_TYPE']} --monitor_name {d3_config['MONITOR_NAME']} --user {d3_config['DKUBEUSERNAME']}\"\n",
    "input_dataset_mount = ['/data']\n",
    "output_dataset = d3_config['RETRAINING_DATASET']\n",
    "output_mount_path = ['/train-data']\n",
    "\n",
    "## Training stage inputs\n",
    "training_script = \"python insurance_datasources/training.py\"\n",
    "model_name = d3_config['MODEL_NAME']\n",
    "output_model_mount = \"/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='training-pipeline',\n",
    "    description='insurance-training-pl'\n",
    ")\n",
    "def insurance_pipeline(token):\n",
    "    \n",
    "    preprocessing = dkube_preprocessing_op(\n",
    "                                    auth_token=str(token),\n",
    "                                    container=json.dumps({\"image\": \"ocdr/d3-datascience-sklearn:v0.23.2-1\"}),\n",
    "                                    program=str(training_program),\n",
    "                                    datasets = json.dumps([str(input_training_dataset)]),\n",
    "                                    input_dataset_mounts = json.dumps(input_dataset_mount),\n",
    "                                    run_script=str(preprocessing_script),\n",
    "                                    outputs=json.dumps([str(output_dataset)]),\n",
    "                                    output_mounts=json.dumps(output_mount_path)).set_display_name(\"data-generation\")\n",
    "    \n",
    "    train       = dkube_training_op(container=json.dumps({\"image\": \"docker.io/ocdr/d3-datascience-sklearn:v0.23.2\"}),\n",
    "                                    framework=\"sklearn\", version=\"0.23.2\",\n",
    "                                    program=str(training_program), \n",
    "                                    run_script=str(training_script),\n",
    "                                    datasets=json.dumps([str(output_dataset)]), outputs=json.dumps([str(model_name)]),\n",
    "                                    input_dataset_mounts=json.dumps(output_mount_path),\n",
    "                                    output_mounts=json.dumps([str(output_model_mount)]),\n",
    "                                    auth_token=token).after(preprocessing)\n",
    "    \n",
    "    serving     = dkube_serving_op(model=train.outputs['artifact'], device='cpu',\n",
    "                                    name=d3_config['MONITOR_NAME'],\n",
    "                                    serving_image=json.dumps({\"image\": \"ocdr/sklearnserver:0.23.2\"}),\n",
    "                                    transformer_image =json.dumps({\"image\": \"docker.io/ocdr/d3-datascience-sklearn:v0.23.2\"}),\n",
    "                                    transformer_project=str(training_program),\n",
    "                                    transformer_code='insurance_datasources/transformer.py', auth_token=token).after(train)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not d3_config['USE_REMOTE_DEPLOYMENT']:\n",
    "    deployment_id = api.get_deployment_id(name=d3_config['MONITOR_NAME'])\n",
    "    if not deployment_id:\n",
    "        client.create_run_from_pipeline_func(insurance_pipeline, arguments={'token':d3_config['TOKEN']})\n",
    "    else:\n",
    "        print(\"Deployment Already Existing, skipping create\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
