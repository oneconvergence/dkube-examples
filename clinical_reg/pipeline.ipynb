{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing kubeflow pipelines client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,json, os, random, string\n",
    "job_class = os.getenv(\"DKUBE_JOB_CLASS\")\n",
    "if not job_class:\n",
    "    !{sys.executable} -m pip install kfp==1.4.0 kfp-server-api==1.2.0 --upgrade >> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import kfp pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kubernetes import client as k8s_client\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_component = '''\n",
    "name: create_dkube_resource\n",
    "description: |\n",
    "    creates dkube resources required for pipeline.\n",
    "metadata:\n",
    "  annotations: {platform: 'Dkube'}\n",
    "  labels: {stage: 'create_dkube_resource', logger: 'dkubepl', wfid: '{{workflow.uid}}', runid: '{{pod.name}}'}\n",
    "inputs:\n",
    "  - {name: token,      type: String,   optional: false,\n",
    "    description: 'Required. Dkube authentication token.'}\n",
    "  - {name: user,      type: String,   optional: false,\n",
    "    description: 'Required. Dkube Logged in User name.'}\n",
    "  - {name: project_id,      type: String,   optional: false,\n",
    "    description: 'Required. Dkube Project name.'}\n",
    "implementation:\n",
    "  container:\n",
    "    image: docker.io/ocdr/dkube-examples-setup:cli-reg\n",
    "    command: ['python3', 'regressionsetup.py']\n",
    "    args: [\n",
    "      --auth_token, {inputValue: token},\n",
    "      --user, {inputValue: user},\n",
    "      --project_id, {inputValue: project_id}\n",
    "    ]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking project and creating if doesn't exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = os.environ.get(\"DKUBE_PROJECT_ID\", \"\")\n",
    "if not project_id:\n",
    "    project_name = \"clinical-reg\"\n",
    "    from dkube.sdk import *\n",
    "    DKUBE_URL = os.getenv(\"DKUBE_URL\")\n",
    "    DKUBE_TOKEN = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\",\"\")\n",
    "    api = DkubeApi(URL=DKUBE_URL,token=DKUBE_TOKEN)\n",
    "    try:\n",
    "        project = DkubeProject(project_name)\n",
    "        res = api.create_project(project)\n",
    "    except Exception as e:\n",
    "        if e.reason.lower()==\"conflict\":\n",
    "            print(f\"Project {project_name} already exists kindly use a different name\")\n",
    "    project_id = res[\"id\"]\n",
    "else:\n",
    "    project_name = os.environ.get(\"DKUBE_PROJECT_NAME\")\n",
    "run_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define e2e regression Pipeline with Dkube components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "from kfp.components._yaml_utils import load_yaml\n",
    "from kfp.components._yaml_utils import dump_yaml\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "import os\n",
    "import json\n",
    "from random import randint\n",
    "\n",
    "def _component(stage, name):\n",
    "    with open('/mnt/dkube/pipeline/components/{}/component.yaml'.format(stage), 'rb') as stream:\n",
    "        cdict = load_yaml(stream)\n",
    "        cdict['name'] = name\n",
    "        cyaml = dump_yaml(cdict)\n",
    "        return components.load_component_from_text(cyaml)\n",
    "        \n",
    "setup_op = kfp.components.load_component(text = setup_component)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='dkube-regression-pl',\n",
    "    description='sample regression pipeline with dkube components'\n",
    ")\n",
    "\n",
    "def d3pipeline(\n",
    "    user,\n",
    "    auth_token,\n",
    "    project_id,\n",
    "    tags,\n",
    "    #Clinical preprocess\n",
    "    clinical_preprocess_script=\"python cli-pre-processing.py\",\n",
    "    clinical_preprocess_datasets=json.dumps([\"clinical\"]),\n",
    "    clinical_preprocess_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    clinical_preprocess_outputs=json.dumps([\"clinical-preprocessed\"]),\n",
    "    clinical_preprocess_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    \n",
    "    #Image preprocess\n",
    "    image_preprocess_script=\"python img-pre-processing.py\",\n",
    "    image_preprocess_datasets=json.dumps([\"images\"]),\n",
    "    image_preprocess_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    image_preprocess_outputs=json.dumps([\"images-preprocessed\"]),\n",
    "    image_preprocess_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    \n",
    "    #Clinical split\n",
    "    clinical_split_script=\"python split.py --datatype clinical\",\n",
    "    clinical_split_datasets=json.dumps([\"clinical-preprocessed\"]),\n",
    "    clinical_split_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    clinical_split_outputs=json.dumps([\"clinical-train\", \"clinical-test\", \"clinical-val\"]),\n",
    "    clinical_split_output_mounts=json.dumps([\"/opt/dkube/outputs/train\", \"/opt/dkube/outputs/test\", \"/opt/dkube/outputs/val\"]),\n",
    "    \n",
    "    #Image split\n",
    "    image_split_script=\"python split.py --datatype image\",\n",
    "    image_split_datasets=json.dumps([\"images-preprocessed\"]),\n",
    "    image_split_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    image_split_outputs=json.dumps([\"images-train\", \"images-test\", \"images-val\"]),\n",
    "    image_split_output_mounts=json.dumps([\"/opt/dkube/outputs/train\", \"/opt/dkube/outputs/test\", \"/opt/dkube/outputs/val\"])\t,\n",
    "    \n",
    "    #RNA split\n",
    "    rna_split_script=\"python split.py --datatype rna\",\n",
    "    rna_split_datasets=json.dumps([\"rna\"]),\n",
    "    rna_split_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    rna_split_outputs=json.dumps([\"rna-train\", \"rna-test\", \"rna-val\"]),\n",
    "    rna_split_output_mounts=json.dumps([\"/opt/dkube/outputs/train\", \"/opt/dkube/outputs/test\", \"/opt/dkube/outputs/val\"]),\n",
    "    \n",
    "    #Training\n",
    "    job_group = 'default',\n",
    "    #Framework. One of tensorflow, pytorch, sklearn\n",
    "    framework = \"tensorflow\",\n",
    "    #Framework version\n",
    "    version = \"2.3.0\",\n",
    "    #In notebook DKUBE_USER_ACCESS_TOKEN is automatically picked up from env variable\n",
    "    #Or any other custom image name can be supplied.\n",
    "    #For custom private images, please input username/password\n",
    "    training_container=json.dumps({'image':'ocdr/dkube-datascience-tf-cpu:v2.3.0-17'}),\n",
    "    #Name of the workspace in dkube. Update accordingly if different name is used while creating a workspace in dkube.\n",
    "    training_program=\"regression\",\n",
    "    #Script to run inside the training container    \n",
    "    training_script=\"python train_nn.py --epochs 5\",\n",
    "    #Input datasets for training. Update accordingly if different name is used while creating dataset in dkube.    \n",
    "    training_datasets=json.dumps([\"clinical-train\", \"clinical-val\", \"images-train\",\n",
    "                                  \"images-val\", \"rna-train\", \"rna-val\"]),\n",
    "    training_input_dataset_mounts=json.dumps([\"/opt/dkube/inputs/train/clinical\", \"/opt/dkube/inputs/val/clinical\",\n",
    "                                      \"/opt/dkube/inputs/train/images\", \"/opt/dkube/inputs/val/images\",\n",
    "                                      \"/opt/dkube/inputs/train/rna\", \"/opt/dkube/inputs/val/rna\"]),\n",
    "    training_outputs=json.dumps([\"regression-model\"]),\n",
    "    training_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    #Request gpus as needed. Val 0 means no gpu, then training_container=docker.io/ocdr/dkube-datascience-tf-cpu:v1.12    \n",
    "    training_gpus=0,\n",
    "    #Any envs to be passed to the training program    \n",
    "    training_envs=json.dumps([{\"steps\": 100}]),\n",
    "    \n",
    "    tuning=json.dumps({}),\n",
    "    \n",
    "    #Evaluation\n",
    "    evaluation_script=\"python evaluate.py\",\n",
    "    evaluation_datasets=json.dumps([\"clinical-test\", \"images-test\", \"rna-test\"]),\n",
    "    evaluation_input_dataset_mounts=json.dumps([\"/opt/dkube/inputs/test/clinical\", \"/opt/dkube/inputs/test/images\",\n",
    "                                      \"/opt/dkube/inputs/test/rna\"]),\n",
    "    evaluation_models=json.dumps([\"regression-model\"]),\n",
    "    evaluation_input_model_mounts=json.dumps([\"/opt/dkube/inputs/model\"]),\n",
    "    \n",
    "    #Serving\n",
    "    #Device to be used for serving - dkube mnist example trained on gpu needs gpu for serving else set this param to 'cpu'\n",
    "    serving_device='cpu',\n",
    "    #Serving image\n",
    "    serving_image=json.dumps({'image':'ocdr/tensorflowserver:2.3.0'}),\n",
    "    #Transformer image\n",
    "    transformer_image=json.dumps({'image':'ocdr/dkube-datascience-tf-cpu:v2.3.0-17'}),\n",
    "    #Script to execute the transformer\n",
    "    transformer_code=\"clinical_reg/transformer.py\"):\n",
    "    \n",
    "    create_resource = setup_op(user = user, token = auth_token, project_id = project_id)\n",
    "    \n",
    "    create_resource.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    clinical_preprocess = _component('preprocess', 'clinical-preprocess')(container=training_container,\n",
    "                                      tags=tags, program=training_program, run_script=clinical_preprocess_script,\n",
    "                                      datasets=clinical_preprocess_datasets, outputs=clinical_preprocess_outputs,\n",
    "                                      input_dataset_mounts=clinical_preprocess_input_mounts, output_mounts=clinical_preprocess_output_mounts).after(create_resource)\n",
    "    image_preprocess  = _component('preprocess', 'images-preprocess')(container=training_container,\n",
    "                                      tags=tags, program=training_program, run_script=image_preprocess_script,\n",
    "                                      datasets=image_preprocess_datasets, outputs=image_preprocess_outputs,\n",
    "                                      input_dataset_mounts=image_preprocess_input_mounts, output_mounts=image_preprocess_output_mounts).after(create_resource)\n",
    "                                      \n",
    "    clinical_split  = _component('preprocess', 'clinical-split')(container=training_container,\n",
    "                                      tags=tags, program=training_program, run_script=clinical_split_script,\n",
    "                                      datasets=clinical_split_datasets, outputs=clinical_split_outputs,\n",
    "                                      input_dataset_mounts=clinical_split_input_mounts,\n",
    "                                      output_mounts=clinical_split_output_mounts).after(clinical_preprocess)\n",
    "                                      \n",
    "    image_split  = _component('preprocess', 'images-split')(container=training_container,\n",
    "                                      tags=tags, program=training_program, run_script=image_split_script,\n",
    "                                      datasets=image_split_datasets, outputs=image_split_outputs,\n",
    "                                      input_dataset_mounts=image_split_input_mounts,\n",
    "                                      output_mounts=image_split_output_mounts).after(image_preprocess)\n",
    "                                      \n",
    "    rna_split  = _component('preprocess', 'rna-split')(container=training_container,\n",
    "                                      tags=tags, program=training_program, run_script=rna_split_script,\n",
    "                                      datasets=rna_split_datasets, outputs=rna_split_outputs,\n",
    "                                      input_dataset_mounts=rna_split_input_mounts, output_mounts=rna_split_output_mounts).after(create_resource)\n",
    "                                      \n",
    "    train       = _component('training', 'regression-model-training')(container=training_container,\n",
    "                                    tags=tags, program=training_program, run_script=training_script,\n",
    "                                    datasets=training_datasets, outputs=training_outputs,\n",
    "                                    input_dataset_mounts=training_input_dataset_mounts,\n",
    "                                    output_mounts=training_output_mounts,\n",
    "                                    ngpus=training_gpus,\n",
    "                                    envs=training_envs,\n",
    "                                    tuning=tuning, job_group=job_group,\n",
    "                                    framework=framework, version=version).after(clinical_split).after(image_split).after(rna_split)\n",
    "    serving     = _component('serving', 'model-serving')(model=train.outputs['artifact'], device=serving_device,\n",
    "                                serving_image=serving_image,\n",
    "                                transformer_image=transformer_image,\n",
    "                                transformer_project=training_program,\n",
    "                                transformer_code=transformer_code).after(train)\n",
    "    inference   = _component('viewer', 'model-inference')(servingurl=serving.outputs['servingurl'],\n",
    "                                 servingexample='regression', viewtype='inference').after(serving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and generate tar ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_filename = 'dkube_regression_pl_full.tar.gz'\n",
    "pipeline_name = 'Regression Pipeline'\n",
    "compiler.Compiler().compile(d3pipeline, pipeline_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_token = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\")\n",
    "client = kfp.Client(host=os.getenv(\"KF_PIPELINES_ENDPOINT\"), existing_token=existing_token, namespace=os.getenv(\"USERNAME\"))\n",
    "try:\n",
    "  client.upload_pipeline(pipeline_package_path = pipeline_filename, pipeline_name = pipeline_name, description = None)\n",
    "except BaseException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))\n",
    "tags = json.dumps([f\"project:{project_id}\"])\n",
    "project_name = os.environ.get(\"DKUBE_PROJECT_NAME\")\n",
    "run_name = f\"[{project_name}] Clinical Reg {run_id}\"\n",
    "experiment = f\"[{project_name}] experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create regression experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.list_experiments()\n",
    "# Create a new experiment\n",
    "try:\n",
    "    clinical_experiment = client.create_experiment(name=experiment)\n",
    "except BaseException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = os.getenv(\"USER\")\n",
    "auth_token = existing_token\n",
    "\n",
    "try:\n",
    "    run = client.create_run_from_pipeline_func(d3pipeline,\n",
    "                                               run_name=run_name,\n",
    "                                               experiment_name=experiment,\n",
    "                                               arguments={\"user\":user,\n",
    "                                                          \"auth_token\":auth_token,\n",
    "                                                          \"project_id\":project_id,\n",
    "                                                          \"tags\":tags})\n",
    "except BaseException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (main, Sep 15 2022, 01:51:29) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
